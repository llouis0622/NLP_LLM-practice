{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-10T13:16:23.896559Z",
     "start_time": "2026-01-10T13:16:23.332213Z"
    }
   },
   "source": [
    "default_installations = True\n",
    "if default_installations:\n",
    "    !pip install -q llmlingua llama-index==0.10.7 accelerate datasets llama-index-postprocessor-longllmlingua\n",
    "else:\n",
    "    import requests\n",
    "\n",
    "    text_file_path = \"requirements__Ch9_RAGLlamaIndex_Prompt_Compression.txt\"\n",
    "    url = \"https://raw.githubusercontent.com/PacktPublishing/Mastering-NLP-from-Foundations-to-LLMs/main/Chapter9_notebooks/\" + text_file_path\n",
    "    res = requests.get(url)\n",
    "    with open(text_file_path, \"w\") as f:\n",
    "        f.write(res.text)\n",
    "    !pip install -r requirements__Ch9_RAGLlamaIndex_Prompt_Compression.txt"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T13:16:23.912692Z",
     "start_time": "2026-01-10T13:16:23.908605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.indices.query.schema import QueryBundle\n",
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "from llama_index.postprocessor.longllmlingua import LongLLMLinguaPostprocessor\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import os"
   ],
   "id": "c2acb0d5fe2cb341",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T13:16:23.921547Z",
     "start_time": "2026-01-10T13:16:23.920261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gpt_type = \"gpt-4o-mini\"\n",
    "num_of_iterations = 60\n",
    "set_seed = 0\n",
    "similarity_top_k = 5\n",
    "target_token = 2 ** 9"
   ],
   "id": "b3171a5a98beebee",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T13:16:23.925949Z",
     "start_time": "2026-01-10T13:16:23.924615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ],
   "id": "205bf3eb13ee7ec9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T13:16:23.934077Z",
     "start_time": "2026-01-10T13:16:23.932834Z"
    }
   },
   "cell_type": "code",
   "source": "openai.api_key = \"...\"",
   "id": "e5415ab1c2ae91f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T13:17:06.379720Z",
     "start_time": "2026-01-10T13:16:23.940274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"ccdv/arxiv-classification\")\n",
    "dataset_train = dataset[\"train\"]\n",
    "dataset_train_df = pd.DataFrame(dataset_train)\n",
    "dataset_train_df_ai = dataset_train_df[dataset_train_df[\"label\"] == 2].reset_index(drop=True)"
   ],
   "id": "375735692575f846",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 28388/28388 [00:01<00:00, 16051.00 examples/s]\n",
      "Generating validation split: 100%|██████████| 2500/2500 [00:00<00:00, 16003.49 examples/s]\n",
      "Generating test split: 100%|██████████| 2500/2500 [00:00<00:00, 15182.08 examples/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T13:17:06.805886Z",
     "start_time": "2026-01-10T13:17:06.408554Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_train_df_ai.head(1).style.set_properties(**{'text-align': 'left'})",
   "id": "29d55e3bc6abcb4e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x315a26c00>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e0997_row0_col0, #T_e0997_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e0997\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e0997_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_e0997_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e0997_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e0997_row0_col0\" class=\"data row0 col0\" >arXiv:1711.06299v1 [cs.LG] 16 Nov 2017\n",
       "\n",
       "Bayesian Best-Arm Identification for Selecting\n",
       "Influenza Mitigation Strategies\n",
       "Pieter Libin1,2 , Timothy Verstraeten1 , Diederik M. Roijers1 , Jelena\n",
       "Grujic1 , Kristof Theys2 , Philippe Lemey2 , and Ann Nowé1\n",
       "1\n",
       "\n",
       "Artificial Intelligence Lab, Department of computer science, Vrije\n",
       "Universiteit Brussel, Brussels, Belgium\n",
       "2\n",
       "KU Leuven University of Leuven, Department of Microbiology\n",
       "and Immunology, Rega Institute for Medical Research, Clinical\n",
       "and Epidemiological Virology, Leuven, Belgium\n",
       "November 20, 2017\n",
       "Abstract\n",
       "Pandemic influenza has the epidemic potential to kill millions of people. While various preventive measures exist (i.a., vaccination and school\n",
       "closures), deciding on strategies that lead to their most effective and efficient use, remains challenging. To this end, individual-based epidemiological models are essential to assist decision makers in determining the\n",
       "best strategy to curve epidemic spread. However, individual-based models are computationally intensive and therefore it is pivotal to identify\n",
       "the optimal strategy using a minimal amount of model evaluations. Additionally, as epidemiological modeling experiments need to be planned,\n",
       "a computational budget needs to be specified a priori. Consequently, we\n",
       "present a new sampling method to optimize the evaluation of preventive\n",
       "strategies using fixed budget best-arm identification algorithms. We use\n",
       "epidemiological modeling theory to derive knowledge about the reward\n",
       "distribution which we exploit using Bayesian best-arm identification algorithms (i.e., Top-two Thompson sampling and BayesGap). We evaluate\n",
       "these algorithms in a realistic experimental setting and demonstrate that\n",
       "it is possible to identify the optimal strategy using only a limited number\n",
       "of model evaluations, i.e., 2-to-3 times faster compared to the uniform\n",
       "sampling method, the predominant technique used for epidemiological\n",
       "decision making in the literature. Finally, we contribute and evaluate a\n",
       "statistic for Top-two Thompson sampling to inform the decision makers\n",
       "about the confidence of an arm recommendation.\n",
       "\n",
       "1\n",
       "\n",
       "\f1\n",
       "\n",
       "Introduction\n",
       "\n",
       "The influenza virus is responsible for the deaths of half of a million people each\n",
       "year [52]. In addition, seasonal influenza epidemics cause a significant economic\n",
       "burden [45]. While transmission is primarily local, a newly emerging variant may\n",
       "spread to pandemic proportions in a naive (i.e., fully susceptible) host population [46]. Pandemic influenza occurs less frequently than seasonal influenza\n",
       "but the outcome with respect to morbidity and mortality can be much more\n",
       "severe, potentially killing millions of people worldwide [47, 48]. Consequently,\n",
       "it is essential to study mitigation strategies to control influenza pandemics.\n",
       "For influenza, different preventive measures exist: i.a., vaccination, social\n",
       "measures (e.g. school closures and travel restrictions) and antiviral drugs. However, the efficiency of strategies greatly depends on the availability of preventive\n",
       "compounds, as well as on the characteristics of the targeted epidemic. Furthermore, governments typically have limited resources to implement such measures.\n",
       "Therefore, it remains challenging to formulate public health strategies that make\n",
       "effective and efficient use of these preventive measures within the existing resource constraints.\n",
       "Epidemiological models (i.e., compartment models and individual-based models) are essential to study the effects of preventive measures in silico [8, 27].\n",
       "While individual-based models are usually associated with a greater model complexity and computational cost than compartment models, they allow for a more\n",
       "accurate evaluation of preventive strategies [12, 19, 44]. To capitalize on these\n",
       "advantages and make it feasible to employ individual-based models, it is essential to use the available computational resources as efficiently as possible.\n",
       "In the literature, a set of possible preventive strategies is typically evaluated\n",
       "by simulating each of the strategies an equal number of times [25, 5, 30, 41, 20,\n",
       "22, 13]. However, this approach is inefficient to identify the optimal preventive\n",
       "strategy, as a large proportion of computational resources will be used to explore\n",
       "sub-optimal strategies. Furthermore, a consensus on the required number of\n",
       "model evaluations per strategy is currently lacking [57]. Moreover, as we show\n",
       "in this paper, this number depends on the hardness of the evaluation problem\n",
       "[6].\n",
       "For this reason, we propose to combine individual-based epidemiological\n",
       "models with multi-armed bandits [32]. In a preliminary study [39], the potential\n",
       "of multi-armed bandits was explored in a regret minimization setting, using default strategies (i.e., ǫ-greedy [53] and UCB1 [7]). However, in this work, we recognize that epidemiological modeling experiments need to be planned and that\n",
       "a computational budget needs to be specified a priori. Within this constraint,\n",
       "we aim to minimize the number of required model evaluations to determine the\n",
       "most promising preventive strategy. Therefore, we present a novel approach\n",
       "formulating the evaluation of preventive strategies as a best-arm identification\n",
       "problem using a fixed budget of model evaluations.\n",
       "As running an individual-based model is computationally intensive (i.e., minutes to hours, depending on the complexity of the model), minimizing the number of required model evaluations reduces the total time required to evaluate\n",
       "2\n",
       "\n",
       "\fa given set of preventive strategies. This renders the use of individual-based\n",
       "models attainable in studies where it would otherwise not be computationally\n",
       "feasible. Additionally, reducing the number of model evaluations can also free\n",
       "up computational resources in studies that already use individual-based models, capacitating researchers to explore different model scenarios. Considering\n",
       "a wider range of scenarios increases the confidence about the overall utility of\n",
       "preventive strategies [58].\n",
       "In our model, an arm’s reward distribution corresponds to the epidemic size\n",
       "distribution of the epidemiological model. We employ epidemiological modeling\n",
       "theory to derive that this distribution is approximately Gaussian, and exploit\n",
       "this knowledge using Bayesian best-arm identification algorithms.\n",
       "In this paper, we contribute a novel method to evaluate preventive strategies\n",
       "as a best-arm identification problem. This method enables decision makers to\n",
       "obtain recommendations in a reduced number of model evaluations and supports\n",
       "their decision process by providing a confidence recommendation statistic. In\n",
       "Section 4 we employ concepts from epidemiological model theory and we adapt\n",
       "Bayesian best-arm identification algorithms to incorporate this knowledge. In\n",
       "Section 5, we evaluate these algorithms in an experimental setting, where we aim\n",
       "to find the best vaccine allocation strategy in a realistic simulation environment\n",
       "that models Seattle’s social network. We repeat the experiment for a wide\n",
       "range of basic reproduction numbers (i.e., R0 , the number of infections that\n",
       "is, by average, generated by one single infection) that are typically used in the\n",
       "influenza literature. The obtained experimental results show that our approach\n",
       "is able to identify the best preventive strategy 2-to-3 times faster compared to\n",
       "uniform sampling, the predominant technique used for epidemiological decision\n",
       "making in the literature. Furthermore, we contribute (Section 4) and evaluate\n",
       "(Section 5) a statistic to inform the decision makers about the confidence of a\n",
       "particular recommendation.\n",
       "\n",
       "2\n",
       "2.1\n",
       "\n",
       "Background\n",
       "Pandemic influenza and vaccine production\n",
       "\n",
       "The primary preventive strategy to mitigate seasonal influenza is to produce\n",
       "vaccine prior to the epidemic, anticipating the virus strains that are expected\n",
       "to circulate. This vaccine pool is used to inoculate the population before the\n",
       "start of the epidemic. While seasonal influenza may have a restricted susceptible\n",
       "population due to vaccination and pre-existing immunity, a newly emerging\n",
       "strain can become pandemic by spreading rapidly among naive human hosts\n",
       "worldwide [46].\n",
       "While it is possible to stockpile vaccines to prepare for seasonal influenza,\n",
       "this is not the case for new variants of influenza viruses, as the vaccine should be\n",
       "specifically tailored to the virus that is the source of the pandemic. Therefore,\n",
       "before an appropriate vaccine can be produced, the responsible virus needs to\n",
       "be identified. Hence, vaccine will be available only in limited supply at the\n",
       "\n",
       "3\n",
       "\n",
       "\fbeginning of the pandemic [56]. In addition, production problems can result\n",
       "in vaccine shortages [18]. When the number of vaccine doses is limited, it is\n",
       "imperative to identify an optimal vaccine allocation strategy [43].\n",
       "\n",
       "2.2\n",
       "\n",
       "Modeling influenza\n",
       "\n",
       "There is a long tradition to use individual-based models to study influenza\n",
       "epidemics [8, 27, 25], as it allows for a more accurate evaluation of preventive\n",
       "strategies. A state-of-the-art individual-based model, that has been the driver\n",
       "for many high impact research efforts [8, 27, 29], is FluTE [12].\n",
       "FluTE implements a contact model where the population is divided into communities of households [12]. The population is organized in a hierarchy of social\n",
       "mixing groups where the contact intensity is inversely proportional with the size\n",
       "of the group (e.g., closer contact between members of a household than between\n",
       "colleagues). Additionally, FluTE implements an individual disease progression\n",
       "model, that associates different disease stages with different levels of infectiousness. To support the evaluation of preventive strategies, FluTE implements\n",
       "the simulation of therapeutic interventions (i.e., vaccines, antiviral compounds)\n",
       "and non-therapeutic interventions (i.e., school closure, case isolation, household\n",
       "quarantine).\n",
       "\n",
       "2.3\n",
       "\n",
       "Bandits and best-arm identification\n",
       "\n",
       "The multi-armed bandit game [6] concerns a K-armed bandit (i.e., a slot machine\n",
       "with K levers), where each arm Ak returns a reward rk when it is pulled (i.e.,\n",
       "rk represents a sample from Ak ’s reward distribution). A common use of the\n",
       "bandit game is to pull a sequence of arms such that the cumulative regret\n",
       "is minimized [32]. To fulfill this goal, the player needs to carefully balance\n",
       "between exploitation (i.e., choose the arms with the highest expected reward)\n",
       "and exploration (i.e., explore the other arms to potentially identify even more\n",
       "promising arms).\n",
       "In this paper, the objective is to recommend the best arm A∗ (i.e., the arm\n",
       "with the highest average reward µ∗ ) after a fixed number of arm pulls. This is\n",
       "referred to as the fixed budget best-arm identification problem [6], an instance\n",
       "of the pure-exploration problem [10]. For a given budget T , the objective is\n",
       "to minimize the simple regret µ∗ − µJ , where µJ is the average reward of the\n",
       "recommended arm AJ at time T [11]. Simple regret is inversely proportional to\n",
       "the probability of recommending the correct arm A∗ [38].\n",
       "\n",
       "3\n",
       "\n",
       "Related work\n",
       "\n",
       "In this work, we recognize that a computational budget needs to be specified\n",
       "a priori to meet the realities associated with high performance computational\n",
       "infrastructure. For this reason we consider the fixed budget best-arm identification setting, in contrast to techniques that attempt to identify the best arm with\n",
       "\n",
       "4\n",
       "\n",
       "\fa predefined confidence: i.e., racing strategies [21], strategies that exploit the\n",
       "confidence bound of the arms’ means [37, 35] and more recently fixed confidence\n",
       "best-arm identification algorithms [26].\n",
       "While other algorithms exist to rank or select bandit arms, e.g. [49], bestarm identification is best approached using adaptive sampling methods [36], as\n",
       "the ones we study in this paper. Moreover, the use of best-arm identification\n",
       "methods clears the way for interesting future work with respect to evaluating\n",
       "preventive strategies while considering multiple objectives (see Section 6).\n",
       "\n",
       "4\n",
       "\n",
       "Methods\n",
       "\n",
       "We formulate the evaluation of preventive strategies as a multi-armed bandit\n",
       "game with the aim of identifying the best arm using a fixed budget of model evaluations. The presented method is generic with respect to the type of epidemic\n",
       "that is modeled (i.e., pathogen, contact network, preventive strategies). The\n",
       "method is evaluated in the context of pandemic influenza in the next section.\n",
       "\n",
       "4.1\n",
       "\n",
       "Preventive bandits\n",
       "\n",
       "Definition 1. A stochastic epidemiological model E is defined in terms of a\n",
       "model configuration c ∈ C and can be used to evaluate a preventive strategy\n",
       "p ∈ P. Evaluating the model E results in a sample of the model’s outcome\n",
       "distribution:\n",
       "outcome ∼ E(c, p), where c ∈ C and p ∈ P\n",
       "(1)\n",
       "Note that a model configuration c ∈ C describes the complete model environment, i.e., both aspects inherent to the model (e.g., FluTE’s mixing model)\n",
       "and options that the modeler can provide (e.g., population statistics, vaccine\n",
       "properties). The result of a model evaluation is referred to as the model outcome\n",
       "(e.g., prevalence, proportion of symptomatic individuals, morbidity, mortality,\n",
       "societal cost).\n",
       "Our objective is to find the optimal preventive strategy from a set of alternative strategies {p1 , ..., pK } ⊂ P for a particular configuration c0 ∈ C of a\n",
       "stochastic epidemiological model, where c0 corresponds to the studied epidemic.\n",
       "Definition 2. A preventive bandit [39] has K = |{p1 , ..., pK }| arms. Pulling\n",
       "arm pk corresponds to evaluating pk by running a simulation in the epidemiological model E(c0 , pk ).\n",
       "A preventive bandit is thus a multi-armed bandit, that has preventive strategies as arms with reward distributions corresponding to the outcome distribution of a stochastic epidemiological model E(c0 , pk ). While the parameters of\n",
       "the reward distribution are known (i.e., the parameters of the epidemiological\n",
       "model), it is intractable to determine the optimal reward analytically from the\n",
       "epidemiological model. Hence, we must learn about the outcome distribution\n",
       "via interaction with the epidemiological model.\n",
       "\n",
       "5\n",
       "\n",
       "\f4.2\n",
       "\n",
       "Outcome distribution\n",
       "\n",
       "As previously defined, the reward distribution associated with a preventive bandit’s arm corresponds to the outcome distribution of the epidemiological model\n",
       "that is evaluated when pulling that arm. Therefore, employing insights from\n",
       "epidemiological modeling theory allows us to specify prior knowledge about the\n",
       "reward distribution.\n",
       "It is well known that a disease outbreak has two possible outcomes: either\n",
       "it is able to spread beyond a local context and becomes a fully established\n",
       "epidemic or it fades out [55]. Most stochastic epidemiological models reflect\n",
       "this reality and hence its epidemic size distribution is bimodal [55]. When\n",
       "evaluating preventive strategies, the objective is to determine the preventive\n",
       "strategy that is most suitable to mitigate an established epidemic. As in practice\n",
       "we can only observe and act on established epidemics, epidemics that faded\n",
       "out in simulation would bias this evaluation. Consequently, it is necessary to\n",
       "focus on the mode of the distribution that is associated with the established\n",
       "epidemic. Therefore we censor (i.e., discard) the epidemic sizes that correspond\n",
       "to the faded epidemic. The size distribution that remains (i.e., the one that\n",
       "corresponds with the established epidemic) is approximately Gaussian [9].\n",
       "In this study, we consider a scaled epidemic size distribution, i.e., the proportion of symptomatic infections. Hence we can assume bimodality of the full\n",
       "size distribution and an approximately Gaussian size distribution of the established epidemic. We verified experimentally that these assumptions hold for all\n",
       "the reward distributions that we observed in our experiments (see Section 5).\n",
       "To censor the size distribution, we use a threshold that represents the number\n",
       "of infectious individuals that are required to ensure an outbreak will only fade\n",
       "out with a low probability.\n",
       "\n",
       "4.3\n",
       "\n",
       "Epidemic fade-out threshold\n",
       "\n",
       "For heterogeneous host populations (i.e., a population with a significant variance\n",
       "among individual transmission rates, as is the case for influenza epidemics [16,\n",
       "24]), the number of secondary infections can be accurately modeled using a\n",
       "negative binomial offspring distribution NB(R0 , γ) [40], where R0 is the basic\n",
       "reproductive number and γ is a dispersion parameter that specifies the extent of\n",
       "heterogeneity. The probability of epidemic extinction pext can be computed by\n",
       "solving g(s) = s, where g(s) is the probability generating function (pgf) of the\n",
       "offspring distribution [40]. For an epidemic where individuals are targeted with\n",
       "preventive measures (i.e., vaccination in our use case), we obtain the following\n",
       "pgf\n",
       "\u0001−γ\n",
       "R0\n",
       "(1 − s)\n",
       "(2)\n",
       "g(s) = popc + (1 − popc ) 1 +\n",
       "γ\n",
       "\n",
       "where popc signifies the random proportion of controlled individuals [40]. From\n",
       "pext we can compute a threshold T0 to limit the probability of extinction to a\n",
       "cutoff ℓ [31].\n",
       "\n",
       "6\n",
       "\n",
       "\f4.4\n",
       "\n",
       "Best-arm identification with a fixed budget\n",
       "\n",
       "Our objective is to identify the best preventive strategy (i.e., the strategy that\n",
       "minimizes the expected outcome) out of a set of preventive strategies, for a\n",
       "particular configuration c0 ∈ C using a fixed budget T of model evaluations.\n",
       "Successive Rejects was the first algorithm to solve the best-arm identification\n",
       "in a fixed budget setting [6]. For a K-armed bandit, Successive Rejects operates\n",
       "in (K − 1) phases. At the end of each phase, the arm with the lowest average\n",
       "reward is discarded. Thus, at the end of phase (K − 1) only one arm survives,\n",
       "and this arm is recommended. At phase f ∈ {1, . . . , K − 1}, each arm that is\n",
       "still available is played mf − mf −1 times, where\n",
       "\n",
       "with\n",
       "\n",
       "m0 = 0\n",
       "\u0006 T −K\n",
       "1 \u0007\n",
       "mf =\n",
       "K + 1 − f log(K)\n",
       "\n",
       "(3)\n",
       "\n",
       "K\n",
       "\n",
       "log(K) =\n",
       "\n",
       "1 X1\n",
       "+\n",
       "2\n",
       "k\n",
       "\n",
       "(4)\n",
       "\n",
       "k=2\n",
       "\n",
       "Successive Rejects serves as a useful baseline, however, it has no support to\n",
       "incorporate any prior knowledge. Bayesian best-arm identification algorithms\n",
       "are able to take into account such knowledge by defining an appropriate prior\n",
       "and posterior on the arms’ reward distribution. As we will show, such prior\n",
       "knowledge can increase the best-arm identification accuracy. Additionally, at\n",
       "the time an arm is recommended, the posteriors contain valuable information\n",
       "that can be used to formulate a variety of statistics helpful to assist decision\n",
       "makers. We consider two state-of-the-art Bayesian algorithms: BayesGap [33]\n",
       "and Top-two Thompson sampling [51]. For Top-two Thompson sampling, we\n",
       "derive a statistic based on the posteriors to inform the decision makers about\n",
       "the confidence of an arm recommendation: the probability of success.\n",
       "As we established in the previous section, each arm of our preventive bandit\n",
       "has a reward distribution that is approximately Gaussian with unknown mean\n",
       "and variance. To make our method generic for any type of preventive bandit\n",
       "problem, we assume an uninformative Jeffreys prior (σk )−3 on (µk , σk2 ) [34].\n",
       "Honda and Takemura [34] demonstrate that this prior leads to the following\n",
       "posterior on µk at the nth\n",
       "k pull:\n",
       "s\n",
       "n2k\n",
       "(5)\n",
       "(µk − xk,nk ) | xk,nk , Sk,nk ∼ Tnk\n",
       "Sk,nk\n",
       "where xk,nk is the reward mean, Sk,nk is the sum of squares\n",
       "Sk,nk =\n",
       "\n",
       "nk\n",
       "X\n",
       "\n",
       "(rk,m − xk,nk )2\n",
       "\n",
       "m=1\n",
       "\n",
       "and Tnk is the standard student t-distribution with nk degrees of freedom.\n",
       "7\n",
       "\n",
       "(6)\n",
       "\n",
       "\fBayesGap is a gap-based Bayesian algorithm [33]. The algorithm requires\n",
       "that for each arm Ak , a high-probability upper bound Uk (t) and lower bound\n",
       "Lk (t) is defined on the posterior of µk at each time step t. Using these bounds,\n",
       "the gap quantity\n",
       "Bk (t) = max Ul (t) − Lk (t)\n",
       "(7)\n",
       "l6=k\n",
       "\n",
       "is defined for each arm Ak . Bk (t) represents an upper bound on the simple regret\n",
       "(as defined in Section 2.3). At each step t of the algorithm, the arm J(t) that\n",
       "minimizes the gap quantity Bk (t) is compared to the arm j(t) that maximizes\n",
       "the upper bound Uk (t). From J(t) and j(t), the arm with the highest confidence\n",
       "diameter Uk (t) − Lk (t) is pulled. The reward that results from this pull is\n",
       "observed and used to update Ak ’s posterior. When the budget is consumed, the\n",
       "arm\n",
       "J(argmin BJ(t) (t))\n",
       "(8)\n",
       "t≤T\n",
       "\n",
       "is recommended. This is the arm that minimizes the simple regret bound over\n",
       "all times t ≤ T .\n",
       "In order to use BayesGap in the preventive bandit setting, we contribute\n",
       "problem-specific bounds. Given our posteriors (Equation 5), we define\n",
       "Uk (t) = µ̂k (t) + β σ̂k (t)\n",
       "Lk (t) = µ̂k (t) − β σ̂k (t)\n",
       "\n",
       "(9)\n",
       "\n",
       "where µ̂k (t) and σ̂k (t) are the mean and standard deviation of the posterior of\n",
       "arm Ak at time step t, and β is the exploration coefficient.\n",
       "The amount of exploration that is feasible given a particular bandit game, is\n",
       "proportional to the available budget, and inversely proportional to the game’s\n",
       "complexity [33]. This complexity can be modeled taking into account the game’s\n",
       "hardness [6] and the variance of the rewards. Following Hoffman et al. [33], we\n",
       "define a hardness quantity\n",
       "X\n",
       "−2\n",
       "Hǫ =\n",
       "Hk,ǫ\n",
       "(10)\n",
       "k\n",
       "\n",
       "with arm-dependent hardness\n",
       "\n",
       "1\n",
       "Hk,ǫ = max( (∆k + ǫ), ǫ)\n",
       "2\n",
       "∆k = max(µl ) − µk\n",
       "\n",
       "(11)\n",
       "\n",
       "l6=k\n",
       "\n",
       "Considering the budget T , hardness Hǫ and a generalized reward variance\n",
       "2\n",
       "σG\n",
       "over all arms, we define\n",
       "s\n",
       "T − 3K\n",
       "(12)\n",
       "β=\n",
       "2\n",
       "4Hǫ σG\n",
       "Theorem 1 in the Supplementary Information1 formally proves that using these\n",
       "bounds results in a probability of simple regret that asymptotically reaches the\n",
       "exponential lower bound presented by Hoffman et al. [33].\n",
       "1 Supplementary\n",
       "\n",
       "Information is available at the end of this manuscript\n",
       "\n",
       "8\n",
       "\n",
       "\f2\n",
       "As both Hǫ and σG\n",
       "are unknown, in order to compute β, these quantities\n",
       "need to be estimated. Firstly, we estimate Hǫ ’s upper bound Ĥǫ by estimating\n",
       "∆k as follows\n",
       "\u0001\n",
       "\u0001\n",
       "ˆ k = max\n",
       "∆\n",
       "µ̂l (t) + 3σ̂l (t) − µ̂k (t) − 3σ̂k (t)\n",
       "(13)\n",
       "1≤l<K;l6=k\n",
       "\n",
       "2\n",
       "as in Hoffman et al. [33]. Secondly, for σG\n",
       "we need a measure of variance that\n",
       "is representative for the reward distribution of all arms. To this end, when the\n",
       "arms are initialized, we observe their sample variance s2k , and compute their\n",
       "average\n",
       "PK 2\n",
       "s\n",
       "2\n",
       "s̄G = k=1 k\n",
       "(14)\n",
       "K\n",
       "As our bounds depend on the standard deviation σ̂k (t) of the t-distributed\n",
       "posterior, each arm’s posterior needs to be initialized 3 times to ensure that\n",
       "σ̂k (t) is defined, this initialization also ensures proper posteriors [34].\n",
       "Top-two Thompson sampling is a reformulation of the Thompson sampling\n",
       "algorithm [54], such that it can be used in a pure-exploration context [51].\n",
       "Thompson sampling operates directly on the arms’ posterior of the mean µk .\n",
       "At each time step, Thompson sampling obtains one sample for each arm’s posterior. The arm with the highest sample is pulled, and its reward is subsequently\n",
       "used to update that arm’s posterior. While this approach has been proven\n",
       "highly successful to minimize cumulative regret [14, 3, 34], as it balances the\n",
       "exploration-exploitation trade-off, it is sub-optimal to identify the best arm [10].\n",
       "To adapt Thompson sampling to minimize simple regret, Top-two Thompson\n",
       "sampling increases the amount of exploration. To this end, an exploration probability ω needs to be specified. At each time step, one sample is obtained for\n",
       "each arm’s posterior. The arm Atop with the highest sample is only pulled with\n",
       "probability ω. With probability 1 − ω we repeat sampling from the posteriors\n",
       "until we find an arm Atop-2 that has the highest posterior sample and where\n",
       "Atop 6= Atop-2 . When the arm Atop-2 is found, it is pulled and the observed\n",
       "reward is used to update the posterior of the pulled arm. When the available\n",
       "budget is consumed, the arm with the highest average reward is recommended.\n",
       "As Top-two Thompson sampling only requires samples from the arms’ posteriors, we can use the t-distributed posteriors from Equation 5 as is. To avoid\n",
       "improper posteriors, each arm needs to be initialized 2 times [34].\n",
       "As specified in the previous subsection, the reward distribution is censored.\n",
       "We observe each reward, but only consider it to update the arm’s value when it\n",
       "exceeds the threshold T0 (i.e., when we receive a sample from the mode of the\n",
       "epidemic that represents the established epidemic).\n",
       "\n",
       "4.5\n",
       "\n",
       "Probability of success\n",
       "\n",
       "The probability that an arm recommendation is correct presents a useful confidence statistic to support policy makers with their decisions. As Top-two\n",
       "Thompson sampling recommends the arm with the highest average reward, the\n",
       "\n",
       "9\n",
       "\n",
       "\fprobability of success is\n",
       "P (µJ = max µk )\n",
       "1≤k≤K\n",
       "\n",
       "(15)\n",
       "\n",
       "where µJ is the random variable that represents the mean of the recommended\n",
       "arm.\n",
       "As we assume that the arm’s reward distributions are independent, this\n",
       "probability can be computed using the recommended arm’s posterior probability density function fµJ and the other arms’ cumulative density function Fµk :\n",
       "P (µJ = max µk ) = P (∩K\n",
       "k6=J (µk ≤ µJ ))\n",
       "1≤k≤K\n",
       "Z\n",
       "=\n",
       "P (∩K\n",
       "k6=J (µk ≤ x))P (µJ = x)dx\n",
       "x∈R\n",
       "\n",
       "=\n",
       "\n",
       "Z\n",
       "\n",
       "x∈R\n",
       "\n",
       "=\n",
       "\n",
       "Z\n",
       "\n",
       "x∈R\n",
       "\n",
       "K\n",
       "\u0002Y\n",
       "\n",
       "k6=J\n",
       "\n",
       "K\n",
       "\u0002Y\n",
       "\n",
       "k6=J\n",
       "\n",
       "\u0003\n",
       "P (µk ≤ x) P (µJ = x)dx\n",
       "\u0003\n",
       "Fµk (x) fµJ (x)dx\n",
       "\n",
       "As this integral cannot be computed analytically, we estimate it using Gaussian\n",
       "quadrature [2].\n",
       "It is important to notice that, while aiming for generality, we made some\n",
       "conservative assumptions: the reward distributions are approximated as Gaussian and the uninformative Jeffreys prior is used. These assumptions imply\n",
       "that the derived probability of success will be an under-estimator for the actual\n",
       "recommendation success.\n",
       "\n",
       "5\n",
       "\n",
       "Experiments\n",
       "\n",
       "We composed and performed an experiment in the context of pandemic influenza, where we analyze the mitigation strategy to vaccinate a population\n",
       "when only a limited number of vaccine doses is available (details about the rationale behind this scenario in Section 2.1). In our experiment, we extend the\n",
       "simulation environment presented in [39] to accommodate a realistic setting to\n",
       "evaluate vaccine allocation. In contrast to [39], we consider a large and realistic\n",
       "social network (i.e., the city of Seattle) and a wide range of R0 values.\n",
       "We consider the scenario when a pandemic is emerging in a particular geographical region and vaccines becomes available, albeit in a limited number of\n",
       "doses. When the number of vaccine doses is limited, it is imperative to identify an optimal vaccine allocation strategy [43]. In our experiment, we explore\n",
       "the allocation of vaccines over five different age groups: pre-school children,\n",
       "school-age children, young adults, older adults and the elderly as presented by\n",
       "Chao et al. [12]. We consider this experiment for a wide range of R0 values.\n",
       "\n",
       "10\n",
       "\n",
       "\f5.1\n",
       "\n",
       "Influenza model and configuration\n",
       "\n",
       "The epidemiological model used in the experiments is the FluTE stochastic\n",
       "individual-based model. In our experiment we consider the population of Seattle\n",
       "(United states) that includes 560,000 individuals [12]. This population is realistic both with respect to the number of individuals and its community structure,\n",
       "and provides an adequate setting for the validation of vaccine strategies [57].\n",
       "At the first day of the simulated epidemic, 10 random individuals are seeded\n",
       "with an infection. The epidemic is simulated for 180 days, during this time no\n",
       "more infections are seeded. Thus, all new infections established during the run\n",
       "time of the simulation, result from the mixing between infectious and susceptible\n",
       "individuals. We assume no pre-existing immunity towards the circulating virus\n",
       "variant. We choose the number of vaccine doses to allocate to be approximately\n",
       "4.5% of the population size [43].\n",
       "In this experiment, we explore the efficacy of different vaccine allocation\n",
       "strategies. We consider that only one vaccine variant is available in the simulation environment. FluTE allows vaccine efficacy to be configured on 3 levels: efficacy to protect against infection when an individual is susceptible (i.e.,\n",
       "V ESus ), efficacy to avoid an infected individual from becoming infectious (i.e.,\n",
       "V EInf ) and efficacy to avoid an infected individual from becoming symptomatic\n",
       "(i.e., V ESym ). In our experiment we choose V ESus = 0.5 [42], V EInf = 0.5 [42]\n",
       "and V ESym = 0.67 [59]. The influenza vaccine only becomes fully effective after\n",
       "a certain period upon its administration, and the effectiveness increases gradually over this period [1]. In our experiment, we assume the vaccine effectiveness\n",
       "to build up exponentially over a period of 2 weeks [1].\n",
       "We perform our experiment for a set of R0 values within the range of 1.4\n",
       "to 2.4, in steps of 0.2. This range is considered representative for the epidemic\n",
       "potential of influenza pandemics [8, 12, 43]. We refer to this set of R0 values as\n",
       "R0 .\n",
       "Note that the setting described in this subsection, in conjunction with a\n",
       "particular R0 value, corresponds to a model configuration (i.e., c0 ∈ C, Definition 2).\n",
       "The computational complexity of FluTE simulations depends both on the\n",
       "size of the susceptible population and the proportion of the population that\n",
       "becomes infected. For the population of Seattle, the simulation run time was up\n",
       "to 11 21 minutes (median of 10 21 minutes), on state-of-the-art hardware (details\n",
       "in Supplementary Information, section 7).\n",
       "\n",
       "5.2\n",
       "\n",
       "Formulating vaccine allocation strategies\n",
       "\n",
       "We consider 5 age groups to which vaccine doses can be allocated: pre-school\n",
       "children (i.e., 0-4 years old), school-age children (i.e., 5-18 years old), young\n",
       "adults (i.e., 19-29 years old), older adults (i.e., 30-64 years old) and the elderly\n",
       "(i.e., > 65 years old) [12]. An allocation scheme can be encoded as a Boolean 5tuple, where each position in the tuple corresponds to the respective age group.\n",
       "The Boolean value at a particular position in the tuple denotes whether vaccines\n",
       "\n",
       "11\n",
       "\n",
       "\fshould be allocated to the respective age group. When vaccine is to be allocated\n",
       "to a particular age group, this is done proportional to the size of the population\n",
       "that is part of this age group [43].\n",
       "To decide on the best vaccine allocation strategy, we enumerate all possible\n",
       "combinations of this tuple. The tuple can be encoded as a binary number, and\n",
       "as such the different allocation strategies can be represented by integers (i.e.,\n",
       "{0, ..., 31}).\n",
       "\n",
       "5.3\n",
       "\n",
       "An influenza preventive bandit\n",
       "\n",
       "The influenza preventive bandit BF lu has exactly 32 arms. Each arm Ak is\n",
       "associated with the allocation strategy for which the integer encoding is k.\n",
       "Given a model configuration c0 ∈ C (Definition 2), when an arm Ak of BF lu\n",
       "is pulled, FluTE is invoked with c0 and the vaccine allocation strategy pk ∈ P\n",
       "(Definition 2) associated with the arm Ak . When FluTE finishes, it outputs the\n",
       "proportion of the population that experienced a symptomatic infection pI , from\n",
       "which the reward rk = 1 − pI is computed.\n",
       "\n",
       "5.4\n",
       "\n",
       "Outcome distributions\n",
       "\n",
       "To establish a proxy for the ground truth concerning the outcome distributions\n",
       "of the 32 considered preventive strategies, all strategies were evaluated 1000\n",
       "times, for each of the R0 values in R0 . We will use this ground truth as a reference to validate the correctness of the recommendations obtained throughout\n",
       "our experiments.\n",
       "R0 presents us with an interesting evaluation problem. To demonstrate this,\n",
       "we visualize the outcome distribution for R0 = 1.4 in Figure 1 and for R0 = 2.4\n",
       "in Figure 2 (the outcome distributions for the other R0 values are shown in\n",
       "Section 3 of the Supplementary Information). Firstly, we observe that for different values of R0 , the distances between top arms’ means differ. Additionally,\n",
       "outcome distribution variances vary over the set of R0 values in R0 . These\n",
       "differences produce distinct levels of evaluation hardness (see Section 4.4), and\n",
       "demonstrate the setting’s usefulness as benchmark to evaluate preventive strategies. Secondly, we expect the outcome distribution to be bimodal, however, the\n",
       "probability to sample from the mode of the outcome distribution that represents\n",
       "the non-established epidemic decreases as R0 increases [40]. This expectation\n",
       "is confirmed when we inspect Figure 1, that shows a bimodal distribution for\n",
       "R0 = 1.4, while Figure 2 shows a unimodal outcome distribution for R0 = 2.4,\n",
       "as only samples from the established epidemic were obtained.\n",
       "Our analysis identified that the best vaccine allocation strategy was h0, 1, 0, 0, 0i\n",
       "(i.e., allocate vaccine to school children, strategy 8) for all R0 values in R0 .\n",
       "\n",
       "5.5\n",
       "\n",
       "Best-arm identification experiment\n",
       "\n",
       "To assess the performance of the different best-arm identification algorithms\n",
       "(i.e., Successive Rejects, BayesGap and Top-two Thompson sampling) we run\n",
       "12\n",
       "\n",
       "\f0.3\n",
       "\n",
       "epidemic size\n",
       "\n",
       "0.2\n",
       "\n",
       "0.1\n",
       "\n",
       "0\n",
       "1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n",
       "6\n",
       "7\n",
       "8\n",
       "9\n",
       "10\n",
       "11\n",
       "12\n",
       "13\n",
       "14\n",
       "15\n",
       "16\n",
       "17\n",
       "18\n",
       "19\n",
       "20\n",
       "21\n",
       "22\n",
       "23\n",
       "24\n",
       "25\n",
       "26\n",
       "27\n",
       "28\n",
       "29\n",
       "30\n",
       "31\n",
       "\n",
       "0.0\n",
       "\n",
       "vaccine allocation strategy\n",
       "Figure 1: Violin plot that depicts the density of the outcome distribution (i.e.,\n",
       "epidemic size) for 32 vaccine allocation strategies (Ro = 1.4).\n",
       "\n",
       "13\n",
       "\n",
       "\fepidemic size\n",
       "\n",
       "0.52\n",
       "\n",
       "0.51\n",
       "\n",
       "0.50\n",
       "\n",
       "0\n",
       "1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n",
       "6\n",
       "7\n",
       "8\n",
       "9\n",
       "10\n",
       "11\n",
       "12\n",
       "13\n",
       "14\n",
       "15\n",
       "16\n",
       "17\n",
       "18\n",
       "19\n",
       "20\n",
       "21\n",
       "22\n",
       "23\n",
       "24\n",
       "25\n",
       "26\n",
       "27\n",
       "28\n",
       "29\n",
       "30\n",
       "31\n",
       "\n",
       "0.49\n",
       "\n",
       "vaccine allocation strategy\n",
       "Figure 2: Violin plot that depicts the density of the outcome distribution (i.e.,\n",
       "epidemic size) for 32 vaccine allocation strategies (Ro = 2.4).\n",
       "\n",
       "14\n",
       "\n",
       "\feach algorithm for all budgets in the range of 32 to 500. This evaluation is\n",
       "performed on the influenza bandit game that we defined earlier. For each budget,\n",
       "we run the algorithms 100 times, and report the recommendation success rate.\n",
       "In the previous section, the optimal vaccine allocation strategy was identified\n",
       "to be h0, 1, 0, 0, 0i (i.e. vaccine allocation strategy 8) for all R0 in R0 . We thus\n",
       "consider a recommendation to be correct when it equals this vaccine allocation\n",
       "strategy.\n",
       "We evaluate the algorithm’s performance with respect to each other and with\n",
       "respect to uniform sampling, the current state-of-the art to evaluate preventive\n",
       "strategies. The uniform sampling method pulls arm Au for each step t of the\n",
       "given budget T , where Au ’s index u is sampled from the uniform distribution\n",
       "U(1, K). To consider different levels of hardness and obtain insight in the effect\n",
       "of the unestablished outcome distribution, we perform this analysis for each R0\n",
       "value in R0 .\n",
       "For the Bayesian best-arm identification algorithms, the prior specifications\n",
       "are detailed in Section 4.4. BayesGap requires an upper and lower bound that\n",
       "is defined in terms of the used posteriors. In our experiments, we use upper\n",
       "bound Uk (t) and lower bound Lk (t) that were established in Section 4.4. Toptwo Thompson sampling requires a parameter that modulates the amount of\n",
       "exploration ω. As it is important for best-arm identification algorithms to differentiate between the top two arms, we choose ω = 0.5, such that, in the limit,\n",
       "Top-two Thompson sampling will explore the top two arms uniformly.\n",
       "We censor the reward distribution based on the threshold T0 we defined\n",
       "in Section 4.3. This threshold depends on basic reproductive number R0 and\n",
       "dispersion parameter γ. R0 is defined explicitly for each of our experimental\n",
       "settings. For the dispersion parameter we choose γ = 0.5, which is a conservative\n",
       "choice according to the literature [16, 24]. We define the probability cutoff\n",
       "ℓ = 10−10 .\n",
       "Figure 3 and Figure 4 show recommendation success rate for each of the\n",
       "best-arm identification algorithms, respectively for R0 = 1.4 and R0 = 2.4. The\n",
       "results for the other R0 values are visualized in Section 4 of the Supplementary\n",
       "Information.\n",
       "The results for different values of R0 clearly indicate that our selection of\n",
       "best-arm identification algorithms significantly outperform the uniform sampling method. In our experiment, where we consider different R0 values, the\n",
       "uniform sampling method requires more than double the amount of evaluations\n",
       "to achieve a similar recommendation performance. For the harder problems\n",
       "(e.g., setting with R0 = 2.4), recommendation uncertainty remains considerable even after consuming 3 times the budget required by Top-two Thompson\n",
       "sampling.\n",
       "All best-arm identification algorithms require an initialization phase in order to output a well-defined recommendation. Successive Rejects needs to pull\n",
       "each arm at least once, while Top-two Thompson sampling and BayesGap need\n",
       "to pull each arm respectively 2 and 3 times (details in Section 4.4). For this\n",
       "reason, these algorithms’ performance can only be evaluated after this initialization phase. BayesGap’s performance is on par with Successive Rejects, except\n",
       "15\n",
       "\n",
       "\ffor the hardest setting we studied (i.e., R0 = 2.4). In comparison, Top-two\n",
       "Thompson sampling consistently outperforms Successive Rejects 30 pulls after\n",
       "the initialization phase.\n",
       "Top-two Thompson sampling needs to initialize each arm’s posterior with\n",
       "2 pulls, i.e., double the amount of uniform sampling and Successive Rejects.\n",
       "However, our experiments clearly show that none of the other algorithms reach\n",
       "any acceptable recommendation rate using less than 64 pulls, thereby alleviating\n",
       "concerns using a t-distributed posterior.\n",
       "\n",
       "1.0\n",
       "\n",
       "success rate\n",
       "\n",
       "0.8\n",
       "0.6\n",
       "0.4\n",
       "BG\n",
       "SR\n",
       "TtTs\n",
       "Uni\n",
       "\n",
       "0.2\n",
       "0.0\n",
       "100\n",
       "\n",
       "200\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "400\n",
       "\n",
       "500\n",
       "\n",
       "Figure 3: In this figure, we present the results for the experiment with R0 =\n",
       "1.4. Each curve represents the rate of successful arm recommendations (y-axis)\n",
       "for a range of budgets (x-axis). A curve is shown for each of the considered\n",
       "algorithms: BayesGap (legend: BG), Successive Rejects (legend: SR), Top-two\n",
       "Thompson sampling (legend: TtTs) and Uniform sampling (legend: Uni).\n",
       "In Section 4 we derived a statistic to express the probability of success (Ps )\n",
       "concerning a recommendation made by Top-two Thompson sampling. We analyzed this probability for all the Top-two Thompson sampling recommendations\n",
       "that were obtained in the experiment described above. To provide some insights\n",
       "on how this statistic can be used to support policy makers, we show the Ps values\n",
       "of all Top-two Thompson sampling recommendations for R0 = 2.4 in Figure 5\n",
       "(Figures for the other R0 values in Section 5 of the Supplementary Information).\n",
       "Figure 5 indicates that Ps closely follows recommendation correctness and that\n",
       "\n",
       "16\n",
       "\n",
       "\f1.0\n",
       "\n",
       "success rate\n",
       "\n",
       "0.8\n",
       "0.6\n",
       "0.4\n",
       "BG\n",
       "SR\n",
       "TtTs\n",
       "Uni\n",
       "\n",
       "0.2\n",
       "0.0\n",
       "100\n",
       "\n",
       "200\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "400\n",
       "\n",
       "500\n",
       "\n",
       "Figure 4: In this figure, we present the results for the experiment with R0 =\n",
       "2.4. Each curve represents the rate of successful arm recommendations (y-axis)\n",
       "for a range of budgets (x-axis). A curve is shown for each of the considered\n",
       "algorithms: BayesGap (legend: BG), Successive Rejects (legend: SR), Top-two\n",
       "Thompson sampling (legend: TtTs) and Uniform sampling (legend: Uni).\n",
       "the uncertainty of Ps is inversely proportional to the size of the available budget.\n",
       "Additionally, in Figure 6 (Figures for the other R0 values in Section 6 of the\n",
       "Supplementary Information) we confirm that Ps underestimates recommendation correctness. These observations indicate that Ps has the potential to serve\n",
       "as a conservative statistic to inform policy makers about the confidence of a particular recommendation, and thus can be used to define meaningful cutoffs to\n",
       "guide policy makers in their interpretation of the recommendation of preventive\n",
       "strategies.\n",
       "\n",
       "6\n",
       "\n",
       "Conclusion\n",
       "\n",
       "We formulate the objective to select the best preventive strategy in an individualbased model as a fixed budget best-arm identification problem. An experiment\n",
       "was set up to evaluate this setting in the context of pandemic influenza. To\n",
       "assess the best arm recommendation performance of the preventive bandit, we\n",
       "report a success rate over 100 independent bandit runs.\n",
       "17\n",
       "\n",
       "\f1.0\n",
       "probability of success\n",
       "\n",
       "0.9\n",
       "0.8\n",
       "0.7\n",
       "0.6\n",
       "0.5\n",
       "0.4\n",
       "\n",
       "success\n",
       "failure\n",
       "\n",
       "0.3\n",
       "100\n",
       "\n",
       "200\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "400\n",
       "\n",
       "500\n",
       "\n",
       "Figure 5: Top-two Thompson sampling was run 100 times for each budget for the\n",
       "experiment with R0 = 2.4. For each of the recommendations, Ps was computed.\n",
       "These Ps values are shown as a scatter plot, where each point’s color reflects\n",
       "the correctness of the recommendation (see legend).\n",
       "We demonstrate that it is possible to efficiently identify the optimal preventive strategy using only a limited number of model evaluations, even if there\n",
       "is a large number of preventive strategies to consider. Compared to uniform\n",
       "sampling, our method is able to recommend the best preventive strategy reducing the number of required model evaluations 2-to-3 times. Additionally, we\n",
       "show that by using Bayesian best-arm identification algorithms, statistics can\n",
       "be defined to support policy makers with their decisions. As such, we are confident that our method has the potential to be used as a decision support tool\n",
       "for mitigating epidemics. This will enable the use of individual-based models in\n",
       "studies where it would otherwise be computationally too prohibitive, and allow\n",
       "researchers to explore a wider variety of model scenarios.\n",
       "We identify two particular directions for future work. Firstly, while our\n",
       "method is evaluated in the context of pandemic influenza, it is important to\n",
       "stress that it can be used to evaluate preventive strategies for other infectious\n",
       "diseases. Since recently, a Dengue vaccine is available [28], and the optimal allocation of this vaccine remains an important research topic [23, 4], we recognize\n",
       "that Dengue epidemics are an interesting use case. Secondly, in this paper, our\n",
       "\n",
       "18\n",
       "\n",
       "\fempirical success rate\n",
       "\n",
       "1.0\n",
       "0.9\n",
       "0.8\n",
       "0.7\n",
       "0.6\n",
       "0.5\n",
       "0.5\n",
       "\n",
       "0.6\n",
       "0.7\n",
       "0.8\n",
       "0.9\n",
       "estimated probability of succes\n",
       "\n",
       "1.0\n",
       "\n",
       "Figure 6: Top-two Thompson sampling was run 100 times for each budget for the\n",
       "experiment with R0 = 2.4. For each of the recommendations, Ps was computed.\n",
       "The Ps values were binned: 0.5 to 1 in steps of 0.05. Per bin, we thus have a set\n",
       "of Bernoulli trials, for which we show the empirical success rate (blue scatter)\n",
       "and the Clopper-Pearson confidence interval (blue confidence bounds) [15]. The\n",
       "orange reference line denotes perfect correlation between the empirical success\n",
       "rate and the estimated probability of success.\n",
       "preventive bandits only learn with respect to a single model outcome (i.e., the\n",
       "proportion of symptomatic infections). However, for many pathogens it is interesting to incorporate multiple objectives (e.g., morbidity, mortality, cost). In\n",
       "the future, we aim to use multi-objective multi-armed bandits [17] in contrast to\n",
       "the current single-objective preventive bandits. With this approach, we plan to\n",
       "learn a coverage set containing an optimal strategy for every possible preference\n",
       "profile the decision makers might have [50].\n",
       "Statement with respect to the reproducibility of our research: if this manuscript\n",
       "is accepted, all source code used in our experiments will be made publicly available.\n",
       "\n",
       "19\n",
       "\n",
       "\fAcknowledgments\n",
       "Pieter Libin was supported by a PhD grant of the FWO (Fonds Wetenschappelijk Onderzoek Vlaanderen) and the VUB research council (VUB/OZR2714).\n",
       "Timothy Verstraeten was supported by a PhD grant of the FWO (Fonds Wetenschappelijk Onderzoek Vlaanderen) and the VUB research council (VUB/OZR2884).\n",
       "Kristof Theys was supported by a postdoctoral grant of the FWO (Fonds Wetenschappelijk Onderzoek Vlaanderen). Diederik Roijers was supported by a postdoctoral grant of the FWO (Fonds Wetenschappelijk Onderzoek Vlaanderen).\n",
       "The computational resources and services used in this work were provided by\n",
       "the Hercules Foundation and the Flemish Government department EWI-FWO\n",
       "Krediet aan Navorsers (Theys, KAN2012 1.5.249.12.).\n",
       "\n",
       "20\n",
       "\n",
       "\fSupplementary Information\n",
       "\n",
       "S1\n",
       "\n",
       "Introduction\n",
       "\n",
       "In this Supplementary Information we provide a proof for BayesGap’s simple\n",
       "regret bound (Section 2). Furthermore, we provide additional figures that were\n",
       "omitted from the main manuscript: figures for the outcome (i.e., epidemic size)\n",
       "distributions (Section 3), figures for the experimental success rates (Section 4),\n",
       "figures for the probabilities of success (i.e., Ps values) per budget (Section 5)\n",
       "and figures for the binned distribution over Ps values (Section 6). Finally, in\n",
       "Section 7, we describe the computational resources that were used to execute\n",
       "the simulations.\n",
       "\n",
       "S2\n",
       "\n",
       "BayesGap simple regret bound for T-distributed\n",
       "posteriors\n",
       "\n",
       "Lemma 1. Consider a Jeffrey’s prior (µk , σk2 ) ∼ σk−3 over the parameters of\n",
       "the Gaussian reward distributions. Then the posterior mean of arm k has the\n",
       "following nonstandardized t-distribution at pull nk :\n",
       "p\n",
       "µk | x̄k,nk , Sk,nk ∼ Tnk (x̄k,nk , n−1\n",
       "Sk,nk )\n",
       "k\n",
       "\n",
       "where nk is the number of pulls for arm k, x̄k,nk is the sample mean and Sk,nk\n",
       "is the sum of squares.\n",
       "Proof. This lemma was presented and proved by Honda et al. [34].\n",
       "\n",
       "Lemma 2. Consider a random variable X ∼ Tν (µ, λ) with variance σ 2 =\n",
       "ν\n",
       "2\n",
       "ν−2 λ , ν > 2 and β > 0. The probability that X is within a radius βσ from its\n",
       "mean can then be written as:\n",
       "P (|X − µ| < βσ) ≥ 1 − 2\n",
       "where\n",
       "C(ν) =\n",
       "\n",
       "ν C(ν)\n",
       "ν−1 β\n",
       "\n",
       "\u0012\n",
       "\u0013−0.5(ν−1)\n",
       "β2\n",
       "1+\n",
       "ν\n",
       "\n",
       "Γ(0.5ν + 0.5)\n",
       "√\n",
       "Γ(0.5ν) πν\n",
       "\n",
       "is the normalizing constant of a standard t-distribution.\n",
       "Proof. Consider a random variable Z ∼ Tν (0, 1), ν > 2 and β > 0. Then the\n",
       "\n",
       "1\n",
       "\n",
       "\fprobability of Z being greater than β\n",
       "P (Z > β\n",
       "\n",
       "r\n",
       "\n",
       "Z\n",
       "\n",
       "ν\n",
       "(1)\n",
       ")=\n",
       "ν−2\n",
       "\n",
       "+∞\n",
       "\n",
       "β\n",
       "\n",
       "√\n",
       "\n",
       "ν\n",
       "ν−2\n",
       "\n",
       "q\n",
       "\n",
       "ν\n",
       "ν−2\n",
       "\n",
       "is:\n",
       "\n",
       "Tν (z | 0, 1)dz\n",
       "\n",
       "\u0012\n",
       "\u0013−0.5(ν+1)\n",
       "z2\n",
       "1\n",
       "+\n",
       "dz\n",
       "√ ν\n",
       "ν\n",
       "β ν−2\n",
       "\u0012\n",
       "\u0013−0.5(ν+1)\n",
       "Z +∞\n",
       "(2)\n",
       "z2\n",
       "z\n",
       "q\n",
       "1+\n",
       "dz\n",
       "≤ C(ν) √\n",
       "ν\n",
       "ν\n",
       "ν\n",
       "β ν−2\n",
       "β ν−2\n",
       "√\n",
       "\u0013−0.5(ν+1)\n",
       "\u0012\n",
       "Z +∞\n",
       "ν−2\n",
       "ν\n",
       "ν −1\n",
       "z2\n",
       "√ C(ν) √\n",
       "=−\n",
       "−\n",
       "z 1+\n",
       "dz\n",
       "ν\n",
       "ν−1 β ν\n",
       "ν\n",
       "ν\n",
       "β ν−2\n",
       "p\n",
       "\u0012\n",
       "\u0013−0.5(ν−1) +∞\n",
       "ν(ν − 2) C(ν)\n",
       "z2\n",
       "(3)\n",
       "1+\n",
       "= −\n",
       "ν−1\n",
       "β\n",
       "ν\n",
       "√ ν\n",
       "\n",
       "= C(ν)\n",
       "\n",
       "Z\n",
       "\n",
       "+∞\n",
       "\n",
       "β\n",
       "\n",
       "ν−2\n",
       "\n",
       "p\n",
       "\u0012\n",
       "\u0013−0.5(ν−1)\n",
       "ν(ν − 2) C(ν)\n",
       "β2\n",
       "(4)\n",
       "1+\n",
       "=\n",
       "ν −1\n",
       "β\n",
       "ν −2\n",
       "q\n",
       "ν\n",
       "The probability of Z being greater than the lower bound β ν−2\n",
       "is the integral\n",
       "\n",
       "over its probability density function, starting from that lower bound (1). In the\n",
       "integral, we introduce a factor β √z ν , which is greater than 1 for the considered\n",
       "ν−2\n",
       "\n",
       "values of z (2). We then take note of the following derivative, and use this result\n",
       "to analytically solve the integral (3):\n",
       "d\n",
       "dx\n",
       "\n",
       "\u0012\n",
       "\u0013−0.5(ν−1)\n",
       "\u0013−0.5(ν+1)\n",
       "\u0012\n",
       "x2\n",
       "x2\n",
       "ν −1\n",
       "1+\n",
       "x 1+\n",
       "=−\n",
       "ν\n",
       "ν\n",
       "ν\n",
       "\n",
       "Finally, we solve the primitive from β √z ν\n",
       "\n",
       "to infinity (4).\n",
       "\n",
       "ν−2\n",
       "\n",
       "Next, we apply a union bound to obtain\n",
       "q a lower bound on the probability\n",
       "ν\n",
       "that the magnitude of Z is smaller than β ν−2\n",
       ":\n",
       "P (|Z| < β\n",
       "\n",
       "r\n",
       "\n",
       "ν\n",
       ")≥1−2\n",
       "ν −2\n",
       "\n",
       "Finally, consider Z =\n",
       "P (|X − µ| < β\n",
       "\n",
       "r\n",
       "\n",
       "p\n",
       "\u0012\n",
       "\u0013−0.5(ν−1)\n",
       "ν(ν − 2) C(ν)\n",
       "β2\n",
       "1+\n",
       "ν −1\n",
       "β\n",
       "ν −2\n",
       "\n",
       "(X−µ)\n",
       ":\n",
       "λ\n",
       "\n",
       "ν\n",
       "λ) ≥ 1 − 2\n",
       "ν −2\n",
       "\n",
       "p\n",
       "\u0012\n",
       "\u0013−0.5(ν−1)\n",
       "ν(ν − 2) C(ν)\n",
       "β2\n",
       "1+\n",
       "ν −1\n",
       "β\n",
       "ν −2\n",
       "\n",
       "Lemma 3. Consider a K-armed bandit problem with budget T and K arms.\n",
       "Let Uk (t) and Lk (t) be upper and lower bounds that hold for all times t ≤ T and\n",
       "2\n",
       "\n",
       "\fall arms k ≤ K with probability 1 − δk (t). Finally, let gk be a monotonically\n",
       "K\n",
       "P\n",
       "gk−1 (Hk,ǫ ) ≤\n",
       "decreasing function such that Uk (t) − Lk (t) ≤ gk (nk (t − 1)) and\n",
       "\n",
       "T − K. We can then bound the simple regret RT as:\n",
       "P (RT < ǫ) ≥ 1 −\n",
       "\n",
       "K X\n",
       "T\n",
       "X\n",
       "\n",
       "k=1\n",
       "\n",
       "δk (t)\n",
       "\n",
       "k=1 t=1\n",
       "\n",
       "Proof. First, we define E as the event in which every mean µk is bounded by its\n",
       "associated bounds (i.e., Uk (t) and Lk (t)) for each time step [33].\n",
       "E := ∀k ≤ K, ∀t ≤ T : Lk (t) ≤ µk ≤ Uk (t)\n",
       "The probability of µk deviating from a single bound at time t is by definition\n",
       "T\n",
       "K P\n",
       "P\n",
       "δk (t).\n",
       "δk (t). When applying the union bound, we obtain P (E) ≥ 1 −\n",
       "k=1 t=1\n",
       "\n",
       "The probability of regret is equal to the probability of the event E occuring, as\n",
       "proven in [33].\n",
       "Theorem 1. Consider a K-armed Gaussian bandit problem with budget T and\n",
       "2\n",
       "unknown variance. Let σG\n",
       "be a generalization of that variance over all arms,\n",
       "and Uk (t) and Lk (t) respectively be the upper and lower bounds for each arm k\n",
       "at time t, where Uk (t) = µ̂k (t) + β σ̂k (t) and Lk (t) = µ̂k (t) − β σ̂k (t). The simple\n",
       "regret is then bounded as:\n",
       "p\n",
       "\u0012\n",
       "\u0013−0.5(nk (t)−1)\n",
       "nk (t)(nk (t) − 2) C(nk (t))\n",
       "β2\n",
       "P (RT ≤ ǫ) ≥ 1 − 2\n",
       "1+\n",
       "nk (t) − 1\n",
       "β\n",
       "nk (t) − 2\n",
       "k=1 t=1\n",
       "\n",
       "\n",
       "\n",
       "−0.5 min nk (t)\n",
       "k,t\n",
       "2\n",
       "β\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "≥ 1 − O KT 1 +\n",
       "\n",
       "min nk (t)\n",
       "K X\n",
       "T\n",
       "X\n",
       "\n",
       "k,t\n",
       "\n",
       "where:\n",
       "\n",
       "β=\n",
       "\n",
       "s\n",
       "\n",
       "T − 3K\n",
       "2\n",
       "4Hǫ σG\n",
       "\n",
       "Note that when min nk (t) → +∞, the bound decreases exponentially in β, simik,t\n",
       "\n",
       "lar to the problem setting presented in [33]. Intuitively, this result makes sense,\n",
       "as for known variances, a Gaussian can be used to describe the posterior means,\n",
       "and indeed, as the number of pulls approaches infinity, our t-distributions converge to Gaussians.\n",
       "Proof. According to Lemma 1, the posterior over the average reward is a t-\n",
       "\n",
       "3\n",
       "\n",
       "\fdistribution with scaling factor λk (t) = nk (t)−1\n",
       "\n",
       "p\n",
       "Sk,nk (t) . Therefore,\n",
       "\n",
       "Uk (t + 1) − Lk (t + 1) = 2β σ̂k (t)\n",
       "p\n",
       "(1)\n",
       "= 2β nk (t)(nk (t) − 2)−1 λk (t)2\n",
       "q\n",
       "(2)\n",
       "= nk (t)(nk (t) − 2)−1 nk (t)−2 Sk,nk (t)\n",
       "s\n",
       "Sk,nk (t)\n",
       "= (nk (t) − 2)−1\n",
       "nk (t)\n",
       "q\n",
       "(3)\n",
       "= (nk (t) − 2)−1 s2k (t)\n",
       "(4)\n",
       "\n",
       "= gk (nk (t))\n",
       "\n",
       "k (t)\n",
       "λk (t)2 for arm k at time t, with\n",
       "The variance of a t-distribution equals nkn(t)−2\n",
       "scaling factor λk (t) as described in Lemma 1 (1 + 2). We denote the variance\n",
       "over rewards per arm as s2k (t) (3) and define gk (nk (t)) to be the upper bound\n",
       "expression as specified in Lemma 3 (4).\n",
       "Next, we compute the inverse of gk (n):\n",
       "\n",
       "gk−1 (m) =\n",
       "\n",
       "4β 2 s2k (t)\n",
       "+2\n",
       "m2\n",
       "\n",
       "2\n",
       "We generalize s2k (t) to a variance σG\n",
       "representative\n",
       "for all arms.2 Approximating\n",
       "P\n",
       "−2\n",
       "the hardness of the problem as Hǫ = k Hk,ǫ , where Hk,ǫ is the arm-dependent\n",
       "hardness defined in [33], we obtain β as follows:\n",
       "K\n",
       "X\n",
       "\n",
       "k=1\n",
       "\n",
       "2\n",
       "gk−1 (Hkǫ ) ≈ 4β 2 σG\n",
       "Hǫ + 2K = T − K\n",
       "\n",
       "⇔β=\n",
       "\n",
       "s\n",
       "\n",
       "T − 3K\n",
       "2\n",
       "4Hǫ σG\n",
       "\n",
       "Finally, as the conditions in Lemma 3 on the function gk are now satisfied,\n",
       "the simple regret bound can be obtained using Lemma 3 and the probability\n",
       "that the true mean is out of the arm-specific bounds Uk (t) and Lk (t), given in\n",
       "Lemma 2.\n",
       "\n",
       "2 In the main paper, we choose σ 2 = s̄2 to be the mean over all arm-specific variances\n",
       "G\n",
       "G\n",
       "obtained after the initialization phase.\n",
       "\n",
       "4\n",
       "\n",
       "\fS3\n",
       "\n",
       "vaccine allocation strategy\n",
       "\n",
       "vaccine allocation strategy\n",
       "\n",
       "0.0\n",
       "\n",
       "0.1\n",
       "\n",
       "0.2\n",
       "\n",
       "0.3\n",
       "\n",
       "Outcome (i.e., epidemic size) distributions\n",
       "0.3\n",
       "\n",
       "0.2\n",
       "\n",
       "0.1\n",
       "\n",
       "0.0\n",
       "\n",
       "(b) Outcome distributions for R0 = 1.6.\n",
       "\n",
       "0.4\n",
       "\n",
       "0.3\n",
       "\n",
       "0.2\n",
       "\n",
       "0.1\n",
       "\n",
       "vaccine allocation strategy\n",
       "\n",
       "0.0\n",
       "\n",
       "0.50\n",
       "\n",
       "0.51\n",
       "\n",
       "0.52\n",
       "\n",
       "(d) Outcome distributions for R0 = 2.0.\n",
       "\n",
       "vaccine allocation strategy\n",
       "\n",
       "(a) Outcome distributions for R0 = 1.4.\n",
       "\n",
       "0.4\n",
       "\n",
       "0.3\n",
       "\n",
       "0.2\n",
       "\n",
       "0.1\n",
       "\n",
       "0.0\n",
       "\n",
       "0.5\n",
       "\n",
       "(c) Outcome distributions for R0 = 1.8.\n",
       "\n",
       "0.4\n",
       "\n",
       "0.3\n",
       "\n",
       "0.2\n",
       "\n",
       "0.49\n",
       "\n",
       "(f) Outcome distributions for R0 = 2.4.\n",
       "\n",
       "vaccine allocation strategy\n",
       "\n",
       "0\n",
       "1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n",
       "6\n",
       "7\n",
       "8\n",
       "9\n",
       "10\n",
       "11\n",
       "12\n",
       "13\n",
       "14\n",
       "15\n",
       "16\n",
       "17\n",
       "18\n",
       "19\n",
       "20\n",
       "21\n",
       "22\n",
       "23\n",
       "24\n",
       "25\n",
       "26\n",
       "27\n",
       "28\n",
       "29\n",
       "30\n",
       "31\n",
       "\n",
       "0.1\n",
       "\n",
       "vaccine allocation strategy\n",
       "\n",
       "(e) Outcome distributions for R0 = 2.2.\n",
       "\n",
       "0.0\n",
       "\n",
       "5\n",
       "\n",
       "0\n",
       "1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n",
       "6\n",
       "7\n",
       "8\n",
       "9\n",
       "10\n",
       "11\n",
       "12\n",
       "13\n",
       "14\n",
       "15\n",
       "16\n",
       "17\n",
       "18\n",
       "19\n",
       "20\n",
       "21\n",
       "22\n",
       "23\n",
       "24\n",
       "25\n",
       "26\n",
       "27\n",
       "28\n",
       "29\n",
       "30\n",
       "31\n",
       "\n",
       "epidemic size\n",
       "\n",
       "0\n",
       "1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n",
       "6\n",
       "7\n",
       "8\n",
       "9\n",
       "10\n",
       "11\n",
       "12\n",
       "13\n",
       "14\n",
       "15\n",
       "16\n",
       "17\n",
       "18\n",
       "19\n",
       "20\n",
       "21\n",
       "22\n",
       "23\n",
       "24\n",
       "25\n",
       "26\n",
       "27\n",
       "28\n",
       "29\n",
       "30\n",
       "31\n",
       "\n",
       "epidemic size\n",
       "\n",
       "epidemic size\n",
       "epidemic size\n",
       "epidemic size\n",
       "\n",
       "epidemic size\n",
       "0\n",
       "1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n",
       "6\n",
       "7\n",
       "8\n",
       "9\n",
       "10\n",
       "11\n",
       "12\n",
       "13\n",
       "14\n",
       "15\n",
       "16\n",
       "17\n",
       "18\n",
       "19\n",
       "20\n",
       "21\n",
       "22\n",
       "23\n",
       "24\n",
       "25\n",
       "26\n",
       "27\n",
       "28\n",
       "29\n",
       "30\n",
       "31\n",
       "0\n",
       "1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n",
       "6\n",
       "7\n",
       "8\n",
       "9\n",
       "10\n",
       "11\n",
       "12\n",
       "13\n",
       "14\n",
       "15\n",
       "16\n",
       "17\n",
       "18\n",
       "19\n",
       "20\n",
       "21\n",
       "22\n",
       "23\n",
       "24\n",
       "25\n",
       "26\n",
       "27\n",
       "28\n",
       "29\n",
       "30\n",
       "31\n",
       "0\n",
       "1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n",
       "6\n",
       "7\n",
       "8\n",
       "9\n",
       "10\n",
       "11\n",
       "12\n",
       "13\n",
       "14\n",
       "15\n",
       "16\n",
       "17\n",
       "18\n",
       "19\n",
       "20\n",
       "21\n",
       "22\n",
       "23\n",
       "24\n",
       "25\n",
       "26\n",
       "27\n",
       "28\n",
       "29\n",
       "30\n",
       "31\n",
       "\n",
       "\fBandit run success rates\n",
       "1.0\n",
       "\n",
       "1.0\n",
       "\n",
       "0.8\n",
       "\n",
       "0.8\n",
       "success rate\n",
       "\n",
       "success rate\n",
       "\n",
       "S4\n",
       "\n",
       "0.6\n",
       "0.4\n",
       "BG\n",
       "SR\n",
       "TtTs\n",
       "Uni\n",
       "\n",
       "0.2\n",
       "0.0\n",
       "100\n",
       "\n",
       "200\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "400\n",
       "\n",
       "100\n",
       "\n",
       "1.0\n",
       "\n",
       "0.8\n",
       "\n",
       "0.8\n",
       "\n",
       "0.6\n",
       "0.4\n",
       "BG\n",
       "SR\n",
       "TtTs\n",
       "Uni\n",
       "\n",
       "0.0\n",
       "100\n",
       "\n",
       "200\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "400\n",
       "\n",
       "100\n",
       "\n",
       "success rate\n",
       "\n",
       "0.8\n",
       "\n",
       "0.4\n",
       "BG\n",
       "SR\n",
       "TtTs\n",
       "Uni\n",
       "400\n",
       "\n",
       "200\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "400\n",
       "\n",
       "500\n",
       "\n",
       "(d) Bandit run results for R0 = 2.0.\n",
       "\n",
       "0.6\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "BG\n",
       "SR\n",
       "TtTs\n",
       "Uni\n",
       "\n",
       "0.0\n",
       "\n",
       "500\n",
       "\n",
       "0.8\n",
       "\n",
       "200\n",
       "\n",
       "500\n",
       "\n",
       "0.4\n",
       "\n",
       "1.0\n",
       "\n",
       "100\n",
       "\n",
       "400\n",
       "\n",
       "0.6\n",
       "\n",
       "1.0\n",
       "\n",
       "0.0\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "0.2\n",
       "\n",
       "(c) Bandit run results for R0 = 1.8.\n",
       "\n",
       "0.2\n",
       "\n",
       "200\n",
       "\n",
       "(b) Bandit run results for R0 = 1.6.\n",
       "\n",
       "1.0\n",
       "\n",
       "0.2\n",
       "\n",
       "BG\n",
       "SR\n",
       "TtTs\n",
       "Uni\n",
       "\n",
       "0.0\n",
       "\n",
       "500\n",
       "\n",
       "success rate\n",
       "\n",
       "success rate\n",
       "\n",
       "0.4\n",
       "0.2\n",
       "\n",
       "(a) Bandit run results for R0 = 1.4.\n",
       "\n",
       "success rate\n",
       "\n",
       "0.6\n",
       "\n",
       "0.6\n",
       "0.4\n",
       "BG\n",
       "SR\n",
       "TtTs\n",
       "Uni\n",
       "\n",
       "0.2\n",
       "0.0\n",
       "\n",
       "500\n",
       "\n",
       "100\n",
       "\n",
       "(e) Bandit run results for R0 = 2.2.\n",
       "\n",
       "200\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "400\n",
       "\n",
       "500\n",
       "\n",
       "(f) Bandit run results for R0 = 2.4.\n",
       "\n",
       "6\n",
       "\n",
       "\fS5\n",
       "\n",
       "Ps values for Top-two Thompson sampling\n",
       "1.0\n",
       "\n",
       "0.9\n",
       "\n",
       "probability of success\n",
       "\n",
       "probability of success\n",
       "\n",
       "1.0\n",
       "\n",
       "0.8\n",
       "0.7\n",
       "0.6\n",
       "0.5\n",
       "\n",
       "success\n",
       "failure\n",
       "\n",
       "0.4\n",
       "100\n",
       "\n",
       "200\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "400\n",
       "\n",
       "0.9\n",
       "0.8\n",
       "0.7\n",
       "\n",
       "500\n",
       "\n",
       "100\n",
       "\n",
       "1.0\n",
       "\n",
       "1.0\n",
       "\n",
       "0.9\n",
       "\n",
       "0.9\n",
       "\n",
       "0.8\n",
       "0.7\n",
       "\n",
       "0.5\n",
       "\n",
       "success\n",
       "failure\n",
       "100\n",
       "\n",
       "200\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "400\n",
       "\n",
       "500\n",
       "\n",
       "100\n",
       "\n",
       "0.9\n",
       "probability of success\n",
       "\n",
       "probability of success\n",
       "\n",
       "200\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "400\n",
       "\n",
       "500\n",
       "\n",
       "(d) Ps values for R0 = 2.0.\n",
       "\n",
       "0.8\n",
       "0.7\n",
       "0.6\n",
       "0.5\n",
       "\n",
       "0.8\n",
       "0.7\n",
       "0.6\n",
       "0.5\n",
       "0.4\n",
       "\n",
       "success\n",
       "failure\n",
       "400\n",
       "\n",
       "success\n",
       "failure\n",
       "\n",
       "0.5\n",
       "\n",
       "1.0\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "500\n",
       "\n",
       "0.6\n",
       "\n",
       "0.9\n",
       "\n",
       "200\n",
       "\n",
       "400\n",
       "\n",
       "0.7\n",
       "\n",
       "1.0\n",
       "\n",
       "100\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "0.8\n",
       "\n",
       "(c) Ps values for R0 = 1.8.\n",
       "\n",
       "0.4\n",
       "\n",
       "200\n",
       "\n",
       "(b) Ps values for R0 = 1.6.\n",
       "\n",
       "probability of success\n",
       "\n",
       "probability of success\n",
       "\n",
       "(a) Ps values for R0 = 1.4.\n",
       "\n",
       "0.6\n",
       "\n",
       "success\n",
       "failure\n",
       "\n",
       "0.6\n",
       "\n",
       "success\n",
       "failure\n",
       "\n",
       "0.3\n",
       "\n",
       "500\n",
       "\n",
       "100\n",
       "\n",
       "(e) Ps values for R0 = 2.2.\n",
       "\n",
       "200\n",
       "\n",
       "300\n",
       "budget\n",
       "\n",
       "400\n",
       "\n",
       "500\n",
       "\n",
       "(f) Ps values for R0 = 2.4.\n",
       "\n",
       "7\n",
       "\n",
       "\fS6\n",
       "\n",
       "Binned distribution of Ps values for Top-two\n",
       "Thompson sampling\n",
       "1.0\n",
       "\n",
       "0.9\n",
       "\n",
       "empirical success rate\n",
       "\n",
       "empirical success rate\n",
       "\n",
       "1.0\n",
       "\n",
       "0.8\n",
       "0.7\n",
       "0.6\n",
       "0.5\n",
       "0.4\n",
       "0.5\n",
       "\n",
       "0.6\n",
       "0.7\n",
       "0.8\n",
       "0.9\n",
       "estimated probability of succes\n",
       "\n",
       "0.4\n",
       "0.2\n",
       "\n",
       "0.5\n",
       "\n",
       "0.6\n",
       "0.7\n",
       "0.8\n",
       "0.9\n",
       "estimated probability of succes\n",
       "\n",
       "1.0\n",
       "\n",
       "(b) Binned distribution for R0 = 1.6.\n",
       "\n",
       "1.0\n",
       "\n",
       "1.0\n",
       "\n",
       "0.8\n",
       "\n",
       "empirical success rate\n",
       "\n",
       "empirical success rate\n",
       "\n",
       "0.6\n",
       "\n",
       "0.0\n",
       "\n",
       "1.0\n",
       "\n",
       "(a) Binned distribution for R0 = 1.4.\n",
       "\n",
       "0.6\n",
       "0.4\n",
       "0.2\n",
       "\n",
       "0.8\n",
       "0.6\n",
       "0.4\n",
       "0.2\n",
       "\n",
       "0.0\n",
       "0.5\n",
       "\n",
       "0.6\n",
       "0.7\n",
       "0.8\n",
       "0.9\n",
       "estimated probability of succes\n",
       "\n",
       "1.0\n",
       "\n",
       "0.5\n",
       "\n",
       "(c) Binned distribution for R0 = 1.8.\n",
       "\n",
       "0.6\n",
       "0.7\n",
       "0.8\n",
       "0.9\n",
       "estimated probability of succes\n",
       "\n",
       "1.0\n",
       "\n",
       "(d) Binned distribution for R0 = 2.0.\n",
       "\n",
       "1.0\n",
       "\n",
       "1.0\n",
       "\n",
       "0.9\n",
       "\n",
       "0.9\n",
       "\n",
       "empirical success rate\n",
       "\n",
       "empirical success rate\n",
       "\n",
       "0.8\n",
       "\n",
       "0.8\n",
       "0.7\n",
       "0.6\n",
       "0.5\n",
       "0.4\n",
       "\n",
       "0.8\n",
       "0.7\n",
       "0.6\n",
       "0.5\n",
       "\n",
       "0.5\n",
       "\n",
       "0.6\n",
       "0.7\n",
       "0.8\n",
       "0.9\n",
       "estimated probability of succes\n",
       "\n",
       "1.0\n",
       "\n",
       "0.5\n",
       "\n",
       "(e) Binned distribution for R0 = 2.2.\n",
       "\n",
       "0.6\n",
       "0.7\n",
       "0.8\n",
       "0.9\n",
       "estimated probability of succes\n",
       "\n",
       "1.0\n",
       "\n",
       "(f) Binned distribution for R0 = 2.4.\n",
       "\n",
       "8\n",
       "\n",
       "\fS7\n",
       "\n",
       "Computational resources\n",
       "\n",
       "The simulations were run on a high performance cluster (HPC). On this HPC,\n",
       "we used “Ivy Bridge” nodes, more specifically nodes with two 10-core ”Ivy\n",
       "Bridge” Xeon E5-2680v2 CPUs (2.8 GHz, 25 MB level 3 cache) and 64 GB of\n",
       "RAM. This infrastructure allowed us to run 20 FluTE simulations per node.\n",
       "\n",
       "9\n",
       "\n",
       "\fReferences\n",
       "[1] Abul K Abbas, Andrew H H Lichtman, and Shiv Pillai. Cellular and\n",
       "molecular immunology. Elsevier Health Sciences, 2014.\n",
       "[2] Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions: with formulas, graphs, and mathematical tables, volume 55. Courier\n",
       "Corporation, 1964.\n",
       "[3] Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the\n",
       "multi-armed bandit problem. In Conference on Learning Theory, pages\n",
       "31–39, 2012.\n",
       "[4] Maira Aguiar and Nico Stollenwerk. Dengvaxia efficacy dependency on\n",
       "serostatus: a closer look at more recent data. Clinical Infectious Diseases,\n",
       "2017.\n",
       "[5] Sigrún Andradóttir, Wenchi Chiu, David Goldsman, Mi Lim Lee, KwokLeung Tsui, Beate Sander, David N Fisman, and Azhar Nizam. Reactive\n",
       "strategies for containing developing outbreaks of pandemic influenza. BMC\n",
       "public health, 11(1):S1, 2011.\n",
       "[6] Jean-Yves Audibert and Sébastien Bubeck. Best arm identification in\n",
       "multi-armed bandits. In COLT-23th Conference on Learning Theory, 2010.\n",
       "[7] Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-time analysis of\n",
       "the multiarmed bandit problem. Machine Learning, 47(2-3):235–256, 2002.\n",
       "ISSN 08856125.\n",
       "[8] Nicole E Basta, Dennis L Chao, M Elizabeth Halloran, Laura Matrajt, and\n",
       "Ira M Longini. Strategies for pandemic and seasonal influenza vaccination\n",
       "of schoolchildren in the United States. American journal of epidemiology,\n",
       "170(6):679–686, 2009.\n",
       "[9] Tom Britton. Stochastic epidemic models: a survey. Mathematical biosciences, 225(1):24–35, 2010.\n",
       "[10] Sébastien Bubeck, Rémi Munos, and Gilles Stoltz. Pure exploration in\n",
       "multi-armed bandits problems. In International conference on Algorithmic\n",
       "learning theory, pages 23–37. Springer, 2009.\n",
       "[11] Sébastien Bubeck, Rémi Munos, and Gilles Stoltz. Pure exploration in\n",
       "finitely-armed and continuous-armed bandits. Theoretical Computer Science, 412(19):1832–1852, 2011.\n",
       "[12] Dennis L Chao, M Elizabeth Halloran, Valerie J Obenchain, and Ira M\n",
       "Longini Jr. FluTE, a publicly available stochastic influenza epidemic simulation model. PLoS Computational Biology, 6(1):e1000656, 2010.\n",
       "\n",
       "10\n",
       "\n",
       "\f[13] Dennis L. Chao, Scott B. Halstead, M. Elizabeth Halloran, and Ira M.\n",
       "Longini. Controlling Dengue with Vaccines in Thailand. PLoS Neglected\n",
       "Tropical Diseases, 6(10), 2012. ISSN 19352727.\n",
       "[14] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in neural information processing systems, pages 2249–\n",
       "2257, 2011.\n",
       "[15] Charles J Clopper and Egon S Pearson. The use of confidence or fiducial\n",
       "limits illustrated in the case of the binomial. Biometrika, pages 404–413,\n",
       "1934.\n",
       "[16] Ilaria Dorigatti, Simon Cauchemez, Andrea Pugliese, and Neil Morris Ferguson. A new approach to characterising infectious disease transmission\n",
       "dynamics from sentinel surveillance: application to the Italian 2009/2010\n",
       "A/H1N1 influenza pandemic. Epidemics, 4(1):9–21, 2012.\n",
       "[17] Madalina M. Drugan and Ann Nowe. Designing multi-objective multiarmed bandits algorithms: A study. In Proceedings of the International\n",
       "Joint Conference on Neural Networks, 2013. ISBN 9781467361293.\n",
       "[18] Martin Enserink. Crisis underscores fragility of vaccine production system.\n",
       "Science, 306(5695):385, 2004.\n",
       "[19] S.G. Eubank, V.S.a. Kumar, M.V. Marathe, A. Srinivasan, and N. Wang.\n",
       "Structure of social contact networks and their impact on epidemics. DIMACS Series in Discrete Mathematics and Theoretical Computer Science,\n",
       "70(0208005):181, 2006. ISSN 1052-1798.\n",
       "[20] Stephen G. Eubank, Hasan Guclu, V S Anil Kumar, Madhav V Marathe,\n",
       "Aravind Srinivasan, Zoltan Toroczkai, and Nan Wang. Modelling disease\n",
       "outbreaks in realistic urban social networks. Nature, 429(6988):180–184,\n",
       "2004.\n",
       "[21] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and\n",
       "stopping conditions for the multi-armed bandit and reinforcement learning\n",
       "problems. Journal of machine learning research, 7(Jun):1079–1105, 2006.\n",
       "[22] Neil M Ferguson, Derek A T Cummings, Simon Cauchemez, Christophe\n",
       "Fraser, and Others. Strategies for containing an emerging influenza pandemic in Southeast Asia. Nature, 437(7056):209, 2005.\n",
       "[23] Neil M Ferguson, Isabel Rodrı́guez-Barraquer, Ilaria Dorigatti, Luis Mier-y\n",
       "Teran-Romero, Daniel J Laydon, and Derek A T Cummings. Benefits and\n",
       "risks of the Sanofi-Pasteur dengue vaccine: Modeling optimal deployment.\n",
       "Science, 353(6303):1033–1036, 2016. ISSN 00429686.\n",
       "[24] Christophe Fraser, Derek A T Cummings, Don Klinkenberg, Donald S\n",
       "Burke, and Neil M Ferguson. Influenza transmission in households during\n",
       "the 1918 pandemic. American journal of epidemiology, 174(5):505–514,\n",
       "2011.\n",
       "11\n",
       "\n",
       "\f[25] Laura Fumanelli, Marco Ajelli, Stefano Merler, Neil M. Ferguson, and Simon Cauchemez. Model-Based Comprehensive Analysis of School Closure\n",
       "Policies for Mitigating Influenza Epidemics and Pandemics. PLoS Computational Biology, 12(1), 2016. ISSN 15537358.\n",
       "[26] Aurélien Garivier and Emilie Kaufmann. Optimal best arm identification\n",
       "with fixed confidence. In Conference on Learning Theory, pages 998–1027,\n",
       "2016.\n",
       "[27] T. C. Germann, K. Kadau, I. M. Longini, and C. A. Macken. Mitigation\n",
       "strategies for pandemic influenza in the United States. Proceedings of the\n",
       "National Academy of Sciences, 103(15):5935–5940, 2006. ISSN 0027-8424.\n",
       "[28] Sri Rezeki Hadinegoro, Jose Luis Arredondo-Garcı́a, Maria Rosario Capeding, Carmen Deseda, Tawee Chotpitayasunondh, Reynaldo Dietze,\n",
       "H.I. Hj Muhammad Ismail, Humberto Reynales, Kriengsak Limkittikul,\n",
       "Doris Maribel Rivera-Medina, Huu Ngoc Tran, Alain Bouckenooghe,\n",
       "Danaya Chansinghakul, Margarita Cortés, Karen Fanouillere, Remi Forrat, Carina Frago, Sophia Gailhardou, Nicholas Jackson, Fernando Noriega, Eric Plennevaux, T. Anh Wartel, Betzana Zambrano, and Melanie\n",
       "Saville. Efficacy and Long-Term Safety of a Dengue Vaccine in Regions of\n",
       "Endemic Disease. New England Journal of Medicine, 373(13):1195–1206,\n",
       "2015. ISSN 0028-4793.\n",
       "[29] M Elizabeth Halloran, Ira M Longini, Azhar Nizam, and Yang Yang. Containing bioterrorist smallpox. Science (New York, N.Y.), 298(5597):1428–\n",
       "1432, 2002. ISSN 00368075.\n",
       "[30] M Elizabeth Halloran, Neil M Ferguson, Stephen Eubank, Ira M Longini,\n",
       "Derek A T Cummings, Bryan Lewis, Shufu Xu, Christophe Fraser, Anil\n",
       "Vullikanti, Timothy C Germann, and Others. Modeling targeted layered\n",
       "containment of an influenza pandemic in the United States. Proceedings of\n",
       "the National Academy of Sciences, 105(12):4639–4644, 2008.\n",
       "[31] Matthew Hartfield and Samuel Alizon. Introducing the outbreak threshold\n",
       "in epidemiology. PLoS Pathog, 9(6):e1003277, 2013.\n",
       "[32] Robbins Herbert. Some Aspects of the Sequential Design of Experiments.\n",
       "Bulletin of the American Mathematical Society, 58(5):527–535, 1952. ISSN\n",
       "02730979.\n",
       "[33] Matthew Hoffman, Bobak Shahriari, and Nando Freitas. On correlation and\n",
       "budget constraints in model-based bandit optimization with application to\n",
       "automatic machine learning. In Artificial Intelligence and Statistics, pages\n",
       "365–374, 2014.\n",
       "[34] Junya Honda and Akimichi Takemura. Optimality of Thompson Sampling\n",
       "for Gaussian Bandits Depends on Priors. In AISTATS, pages 375–383,\n",
       "2014.\n",
       "12\n",
       "\n",
       "\f[35] Kevin Jamieson, Matthew Malloy, Robert Nowak, and Sébastien Bubeck.\n",
       "lil’ucb: An optimal exploration algorithm for multi-armed bandits. In Conference on Learning Theory, pages 423–439, 2014.\n",
       "[36] Christopher Jennison, Iain M Johnstone, and Bruce W Turnbull. Asymptotically optimal procedures for sequential adaptive selection of the best of\n",
       "several normal means. Statistical decision theory and related topics III, 2:\n",
       "55–86, 1982.\n",
       "[37] Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity\n",
       "in bandit subset selection. In Conference on Learning Theory, pages 228–\n",
       "251, 2013.\n",
       "[38] Emilie Kaufmann, Olivier Cappé, and Aurélien Garivier. On the complexity of best arm identification in multi-armed bandit models. Journal of\n",
       "Machine Learning Research, 17(1):1–42, 2016.\n",
       "[39] Pieter Libin, Timothy Verstraeten, Kristof Theys, Diederik Roijers, Peter\n",
       "Vrancx, and Ann Nowe. Efficient evaluation of influenza mitigation strategies using preventive bandits. AAMAS 2017 Visionary Papers, Lecture\n",
       "Notes in AI, Volume 10643, page In press, 2017.\n",
       "[40] James O Lloyd-Smith, Sebastian J Schreiber, P Ekkehard Kopp, and\n",
       "Wayne M Getz. Superspreading and the effect of individual variation on\n",
       "disease emergence. Nature, 438(7066):355–359, 2005.\n",
       "[41] Ira M Longini Jr, M Elizabeth Halloran, Azhar Nizam, and Yang Yang.\n",
       "Containing pandemic influenza with antiviral agents. American journal of\n",
       "epidemiology, 159(7):623–633, 2004.\n",
       "[42] Huong Q. McLean, Mark G. Thompson, Maria E. Sundaram, Burney A. Kieke, Manjusha Gaglani, Kempapura Murthy, Pedro A. Piedra,\n",
       "Richard K. Zimmerman, Mary Patricia Nowalk, Jonathan M. Raviotta,\n",
       "Michael L. Jackson, Lisa Jackson, Suzanne E. Ohmit, Joshua G. Petrie,\n",
       "Arnold S. Monto, Jennifer K. Meece, Swathi N. Thaker, Jessie R. Clippard, Sarah M. Spencer, Alicia M. Fry, and Edward A. Belongia. Influenza\n",
       "vaccine effectiveness in the United States during 2012-2013: Variable protection by age and virus type. Journal of Infectious Diseases, 211(10):\n",
       "1529–1540, 2015. ISSN 15376613.\n",
       "[43] Jan Medlock and Alison P Galvani. Optimizing influenza vaccine distribution. Science, 325(5948):1705–1708, 2009. ISSN 0036-8075.\n",
       "[44] Lauren Ancel Meyers, M. E J Newman, Michael Martin, and Stephanie\n",
       "Schrag. Applying network theory to epidemics: Control measures for Mycoplasma pneumoniae outbreaks. Emerging Infectious Diseases, 9(2):204–\n",
       "210, 2003. ISSN 10806040.\n",
       "\n",
       "13\n",
       "\n",
       "\f[45] N. A M Molinari, Ismael R. Ortega-Sanchez, Mark L. Messonnier,\n",
       "William W. Thompson, Pascale M. Wortley, Eric Weintraub, and Carolyn B. Bridges. The annual impact of seasonal influenza in the US: Measuring disease burden and costs. Vaccine, 25(27):5086–5096, 2007. ISSN\n",
       "0264410X.\n",
       "[46] Henry Nicholls. Pandemic influenza: the inside story. PLoS Biol, 4(2):e50,\n",
       "2006. ISSN 1545-7885.\n",
       "[47] K David Patterson and Gerald F Pyle. The geography and mortality of\n",
       "the 1918 influenza pandemic. Bulletin of the History of Medicine, 65(1):4,\n",
       "1991.\n",
       "[48] Catharine Paules and Kanta Subbarao. Influenza. The Lancet, pages –,\n",
       "2017. ISSN 0140-6736.\n",
       "[49] Warren B Powell and Ilya O Ryzhov. Optimal learning, volume 841. John\n",
       "Wiley & Sons, 2012.\n",
       "[50] Diederik M. Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. A survey of multi-objective sequential decision-making. Journal of\n",
       "Artificial Intelligence Research, 48:67–113, 2013. ISSN 10769757.\n",
       "[51] Daniel Russo. Simple bayesian algorithms for best arm identification. In\n",
       "Conference on Learning Theory, pages 1417–1418, 2016.\n",
       "[52] Klaus Stöhr. Influenza: WHO cares. The Lancet infectious diseases, 2(9):\n",
       "517, 2002.\n",
       "[53] R S Sutton and A G Barto. Reinforcement learning: an introduction. 1998.\n",
       "ISBN 0262193981.\n",
       "[54] William R Thompson. On the likelihood that one unknown probability\n",
       "exceeds another in view of the evidence of two samples. Biometrika, 25\n",
       "(3/4):285–294, 1933.\n",
       "[55] Duncan J Watts, Roby Muhamad, Daniel C Medina, and Peter S Dodds.\n",
       "Multiscale, resurgent epidemics in a hierarchical metapopulation model.\n",
       "Proceedings of the National Academy of Sciences of the United States of\n",
       "America, 102(32):11157–11162, 2005.\n",
       "[56] WHO. WHO guidelines on the use of vaccines and antivirals during influenza pandemics. 2004.\n",
       "[57] Lander Willem, Sean Stijven, Ekaterina Vladislavleva, Jan Broeckhove,\n",
       "Philippe Beutels, and Niel Hens. Active Learning to Understand Infectious\n",
       "Disease Models and Improve Policy Making. PLoS Comput Biol, 10(4):\n",
       "e1003563, 2014.\n",
       "\n",
       "14\n",
       "\n",
       "\f[58] Joseph T Wu, Steven Riley, Christophe Fraser, and Gabriel M Leung. Reducing the impact of the next influenza pandemic using household-based\n",
       "public health interventions. PLoS medicine, 3(9):e361, 2006.\n",
       "[59] Wan Yang, Jonathan D Sugimoto, M Elizabeth Halloran, Nicole E Basta,\n",
       "Dennis L Chao, Laura Matrajt, Gail Potter, Eben Kenah, and Ira M\n",
       "Longini. The transmissibility and control of pandemic influenza A (H1N1)\n",
       "virus. Science (New York, N.Y.), 326(2009):729–33, 2009. ISSN 1095-9203.\n",
       "\n",
       "15\n",
       "\n",
       "\f</td>\n",
       "      <td id=\"T_e0997_row0_col1\" class=\"data row0 col1\" >2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T13:17:06.882358Z",
     "start_time": "2026-01-10T13:17:06.864191Z"
    }
   },
   "cell_type": "code",
   "source": "articles = [Document(text=content) for content in dataset_train_df_ai[\"text\"]]",
   "id": "2e3e8ba72406d490",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T13:17:06.891524Z",
     "start_time": "2026-01-10T13:17:06.890045Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"'articles'는 리스트이며, 각 요소는 다음과 같은 유형입니다.:\", type(articles[0]))",
   "id": "eff8f586b937e8e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'articles'는 리스트이며, 각 요소는 다음과 같은 유형입니다.: <class 'llama_index.core.schema.Document'>\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T13:21:46.200121Z",
     "start_time": "2026-01-10T13:21:44.942602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = OpenAI(model=gpt_type)\n",
    "postprocessor = LongLLMLinguaPostprocessor(\n",
    "    model_name=\"NousResearch/Llama-2-7b-hf\",\n",
    "    target_token=target_token,\n",
    "    rank_method=\"longllmlingua\",\n",
    "    additional_compress_kwargs={\n",
    "        \"condition_compare\": True,\n",
    "        \"condition_in_question\": \"after\",\n",
    "        \"context_budget\": \"+100\",\n",
    "        \"reorder_context\": \"sort\",\n",
    "        \"dynamic_context_compression_ratio\": 0.3,\n",
    "    },\n",
    ")"
   ],
   "id": "f1af1ca6dff0d6de",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m llm \u001B[38;5;241m=\u001B[39m OpenAI(model\u001B[38;5;241m=\u001B[39mgpt_type)\n\u001B[0;32m----> 2\u001B[0m postprocessor \u001B[38;5;241m=\u001B[39m \u001B[43mLongLLMLinguaPostprocessor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mNousResearch/Llama-2-7b-hf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrank_method\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlongllmlingua\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43madditional_compress_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcondition_compare\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcondition_in_question\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mafter\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontext_budget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m+100\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreorder_context\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msort\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdynamic_context_compression_ratio\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/pyconda/lib/python3.12/site-packages/llama_index/postprocessor/longllmlingua/base.py:56\u001B[0m, in \u001B[0;36mLongLLMLinguaPostprocessor.__init__\u001B[0;34m(self, model_name, device_map, model_config, open_api_config, metadata_mode, instruction_str, target_token, rank_method, additional_compress_kwargs)\u001B[0m\n\u001B[1;32m     53\u001B[0m open_api_config \u001B[38;5;241m=\u001B[39m open_api_config \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m     54\u001B[0m additional_compress_kwargs \u001B[38;5;241m=\u001B[39m additional_compress_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_llm_lingua \u001B[38;5;241m=\u001B[39m \u001B[43mPromptCompressor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m    \u001B[49m\u001B[43mopen_api_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mopen_api_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     63\u001B[0m     metadata_mode\u001B[38;5;241m=\u001B[39mmetadata_mode,\n\u001B[1;32m     64\u001B[0m     instruction_str\u001B[38;5;241m=\u001B[39minstruction_str,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     67\u001B[0m     additional_compress_kwargs\u001B[38;5;241m=\u001B[39madditional_compress_kwargs,\n\u001B[1;32m     68\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/pyconda/lib/python3.12/site-packages/llmlingua/prompt_compressor.py:89\u001B[0m, in \u001B[0;36mPromptCompressor.__init__\u001B[0;34m(self, model_name, device_map, model_config, open_api_config, use_llmlingua2, llmlingua2_config)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprefix_bos_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moai_tokenizer \u001B[38;5;241m=\u001B[39m tiktoken\u001B[38;5;241m.\u001B[39mencoding_for_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt-3.5-turbo\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 89\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_llmlingua2:\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_llmlingua2(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mllmlingua2_config)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/pyconda/lib/python3.12/site-packages/llmlingua/prompt_compressor.py:140\u001B[0m, in \u001B[0;36mPromptCompressor.load_model\u001B[0;34m(self, model_name, device_map, model_config)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    135\u001B[0m     device_map\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(key \u001B[38;5;129;01min\u001B[39;00m device_map \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    138\u001B[0m )\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map:\n\u001B[0;32m--> 140\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mMODEL_CLASS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtorch_dtype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    151\u001B[0m     model \u001B[38;5;241m=\u001B[39m MODEL_CLASS\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    152\u001B[0m         model_name,\n\u001B[1;32m    153\u001B[0m         device_map\u001B[38;5;241m=\u001B[39mdevice_map,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    156\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_config,\n\u001B[1;32m    157\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/pyconda/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    602\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mconfig_class \u001B[38;5;241m==\u001B[39m config\u001B[38;5;241m.\u001B[39msub_configs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    603\u001B[0m         config \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget_text_config()\n\u001B[0;32m--> 604\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    608\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    609\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    610\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/pyconda/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001B[0m, in \u001B[0;36mrestore_default_dtype.<locals>._wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m old_dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mget_default_dtype()\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    279\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_default_dtype(old_dtype)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/pyconda/lib/python3.12/site-packages/transformers/modeling_utils.py:5048\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   5038\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   5039\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[1;32m   5041\u001B[0m     (\n\u001B[1;32m   5042\u001B[0m         model,\n\u001B[1;32m   5043\u001B[0m         missing_keys,\n\u001B[1;32m   5044\u001B[0m         unexpected_keys,\n\u001B[1;32m   5045\u001B[0m         mismatched_keys,\n\u001B[1;32m   5046\u001B[0m         offload_index,\n\u001B[1;32m   5047\u001B[0m         error_msgs,\n\u001B[0;32m-> 5048\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5049\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5050\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5051\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5052\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5053\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5054\u001B[0m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5055\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5056\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5057\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5058\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5059\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5060\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5061\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5062\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5063\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5064\u001B[0m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[1;32m   5065\u001B[0m model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/pyconda/lib/python3.12/site-packages/transformers/modeling_utils.py:5432\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001B[0m\n\u001B[1;32m   5430\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_hqq_or_quark:\n\u001B[1;32m   5431\u001B[0m     expanded_device_map \u001B[38;5;241m=\u001B[39m expand_device_map(device_map, expected_keys)\n\u001B[0;32m-> 5432\u001B[0m     \u001B[43mcaching_allocator_warmup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpanded_device_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5434\u001B[0m \u001B[38;5;66;03m# Prepare and compatabilize arguments for serial and parallel shard loading\u001B[39;00m\n\u001B[1;32m   5435\u001B[0m args_list \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   5436\u001B[0m     (\n\u001B[1;32m   5437\u001B[0m         shard_file,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5452\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m shard_file \u001B[38;5;129;01min\u001B[39;00m checkpoint_files\n\u001B[1;32m   5453\u001B[0m ]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/pyconda/lib/python3.12/site-packages/transformers/modeling_utils.py:6089\u001B[0m, in \u001B[0;36mcaching_allocator_warmup\u001B[0;34m(model, expanded_device_map, hf_quantizer)\u001B[0m\n\u001B[1;32m   6087\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxpu\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m   6088\u001B[0m     torch_accelerator_module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(torch, device\u001B[38;5;241m.\u001B[39mtype)\n\u001B[0;32m-> 6089\u001B[0m     index \u001B[38;5;241m=\u001B[39m device\u001B[38;5;241m.\u001B[39mindex \u001B[38;5;28;01mif\u001B[39;00m device\u001B[38;5;241m.\u001B[39mindex \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mtorch_accelerator_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6090\u001B[0m     device_memory \u001B[38;5;241m=\u001B[39m torch_accelerator_module\u001B[38;5;241m.\u001B[39mmem_get_info(index)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   6091\u001B[0m     \u001B[38;5;66;03m# Allow up to (max device memory - 1.2 GiB) in resource-constrained hardware configurations. Trying to reserve more\u001B[39;00m\n\u001B[1;32m   6092\u001B[0m     \u001B[38;5;66;03m# than that amount might sometimes lead to unnecessary cuda/xpu OOM, if the last parameter to be loaded on the device is large,\u001B[39;00m\n\u001B[1;32m   6093\u001B[0m     \u001B[38;5;66;03m# and the remaining reserved memory portion is smaller than the param size -> torch will then try to fully re-allocate all\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   6096\u001B[0m     \u001B[38;5;66;03m# Note that we use an absolute value instead of device proportion here, as a 8GiB device could still allocate too much\u001B[39;00m\n\u001B[1;32m   6097\u001B[0m     \u001B[38;5;66;03m# if using e.g. 90% of device size, while a 140GiB device would allocate too little\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/pyconda/lib/python3.12/site-packages/torch/cuda/__init__.py:1069\u001B[0m, in \u001B[0;36mcurrent_device\u001B[0;34m()\u001B[0m\n\u001B[1;32m   1067\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcurrent_device\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m   1068\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Return the index of a currently selected device.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1069\u001B[0m     \u001B[43m_lazy_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1070\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_cuda_getDevice()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/pyconda/lib/python3.12/site-packages/torch/cuda/__init__.py:403\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    398\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    399\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    400\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    401\u001B[0m     )\n\u001B[1;32m    402\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 403\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    404\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    405\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    406\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    407\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = \"Does this publication involve Reinforcement Learning? Answer in a single word, either Yes or No\"\n",
    "record = {\n",
    "    \"Yes,Yes\": 0,\n",
    "    \"No,No\": 0,\n",
    "    \"Yes,No\": 0,\n",
    "    \"No,Yes\": 0,\n",
    "    \"original_tokens\": [],\n",
    "    \"compressed_tokens\": [],\n",
    "    \"ratios\": [],\n",
    "    \"time_nc\": [],\n",
    "    \"time_c\": [],\n",
    "}\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "random.seed(set_seed)\n",
    "random_iterations = random.sample(range(len(articles)), num_of_iterations)\n",
    "iteration_counter = 1\n",
    "for document_index in random_iterations:\n",
    "    index = VectorStoreIndex.from_documents(articles[document_index:(document_index + 1)])\n",
    "    print(\"\\n\\n                                                 문서 인덱스:\", document_index)\n",
    "    print(\"                                                 반복 횟수:\", iteration_counter, \"/\", num_of_iterations)\n",
    "    iteration_counter += 1\n",
    "    retrieval_start_time = time.time()\n",
    "    retriever = index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "    retrieved_context = retriever.retrieve(query)\n",
    "    context_list = [chunk.get_content() for chunk in retrieved_context]\n",
    "    retrieval_time = time.time() - retrieval_start_time\n",
    "    start_time_nc = time.time()\n",
    "    prompt = \"\\n\\n\".join(context_list + [query])\n",
    "    response_nc = llm.complete(prompt)\n",
    "    time_nc = time.time() - start_time_nc\n",
    "    print(\"------------------------------------------------ 비압축 방식 응답: \" + str(response_nc))\n",
    "    original_contexts = \"\\n\\n\".join(context_list)\n",
    "    original_tokens = postprocessor._llm_lingua.get_token_length(original_contexts)\n",
    "    del prompt\n",
    "    del context_list\n",
    "    del original_contexts\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    start_time_c = time.time()\n",
    "    new_retrieved_context = postprocessor.postprocess_nodes(\n",
    "        retrieved_context,\n",
    "        query_bundle=QueryBundle(query_str=query)\n",
    "    )\n",
    "    response_c = CompactAndRefine().synthesize(query, new_retrieved_context)\n",
    "    time_c = time.time() - start_time_c\n",
    "    print(\"------------------------------------------------ 압축 방식 응답: \" + str(response_c))\n",
    "    compressed_contexts = \"\\n\\n\".join([chunk.get_content() for chunk in new_retrieved_context])\n",
    "    compressed_tokens = postprocessor._llm_lingua.get_token_length(compressed_contexts)\n",
    "    ratio = original_tokens / (compressed_tokens + 1)\n",
    "    print(\"원래 토큰 수:\", original_tokens)\n",
    "    print(\"압축된 토큰 수:\", compressed_tokens)\n",
    "    print(\"압축 비율:\", f\"{ratio:.2f}배\")\n",
    "    record[str(response_nc).replace(\".\", \"\") + \",\" + str(response_c).replace(\".\", \"\")] += 1\n",
    "    record[\"original_tokens\"].append(original_tokens)\n",
    "    record[\"compressed_tokens\"].append(compressed_tokens)\n",
    "    record[\"ratios\"].append(ratio)\n",
    "    record[\"time_nc\"].append(time_nc + retrieval_time)\n",
    "    record[\"time_c\"].append(time_c + retrieval_time)\n",
    "    del compressed_contexts\n",
    "    del new_retrieved_context\n",
    "    del retrieved_context\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "e1d77d8a98c5fba3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_of_iterations_in_practice = len(record[\"original_tokens\"])\n",
    "counts_of_agreements = record[\"Yes,Yes\"] + record[\"No,No\"]\n",
    "counts_of_disagreements = record[\"Yes,No\"] + record[\"No,Yes\"]\n",
    "reduction_in_tokens = round(100 * (1 - sum(record[\"compressed_tokens\"]) / sum(record[\"original_tokens\"])))"
   ],
   "id": "91656a384a67a96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"일치 횟수:\", str(counts_of_agreements), \"전체 사례\", str(num_of_iterations_in_practice), \"중\")\n",
    "print(\"불일치 횟수:\", str(counts_of_disagreements), \"전체 사례\", str(num_of_iterations_in_practice), \"중\")\n",
    "print(f\"일치율: {round(100 * counts_of_agreements / num_of_iterations_in_practice)}%\")"
   ],
   "id": "6dd7b4a1dd6a1c84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"비압축 방식: {num_of_iterations_in_practice}번 호출에서 전송된 총 토큰 수:\", str(sum(record[\"original_tokens\"])))\n",
    "print(f\"압축 방식:   {num_of_iterations_in_practice}번 호출에서 전송된 총 토큰 수:\", str(sum(record[\"compressed_tokens\"])))\n",
    "print(f\"토큰 감소율: {reduction_in_tokens}%\")\n",
    "print(\"압축 비율:\", f\"{1 / (1 - reduction_in_tokens / 100):.2f}배\")"
   ],
   "id": "ab73dcdd4208a366"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"비압축 방식: {num_of_iterations_in_practice}번 호출 동안의 총 반복 시간:\", str(round(sum(record[\"time_nc\"]))))\n",
    "print(f\"압축 방식:   {num_of_iterations_in_practice}번 호출 동안의 총 반복 시간:\", str(round(sum(record[\"time_c\"]))))"
   ],
   "id": "35b46b7dc658e309"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "reduction_in_agreement = round(100 * (record[\"Yes,No\"] + record[\"No,Yes\"]) / num_of_iterations_in_practice)\n",
    "print(f\"분류 일치율이 {reduction_in_agreement}% 감소하는 동안, 토큰 감소로 인한 비용 절감율은 {reduction_in_tokens}%입니다.\\n\")\n",
    "print(f\"즉, 압축 전 $100의 비용이 압축 후에는 ${100 - reduction_in_tokens}로 줄어듭니다.\")\n",
    "print(\"*두 접근 방식 간의 불일치는 어느 한쪽이 올바른 결과일 수 있음을 의미하며, 압축되지 않은 방식이 더 신뢰할 수 있다고 가정해서는 안 됩니다. 압축되지 않은 방식 또한 희소할 수 있기 때문입니다.\")\n",
    "print(\"**압축기 사용에는 GPU가 필요한 로컬 LLM 활용과 같은 다른 절충이 따른다는 점을 기억하세요.\")"
   ],
   "id": "9ee8df68d0b591bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "out = open(\"record.pickle\", \"wb\")\n",
    "pickle.dump(record, out)\n",
    "out.close()"
   ],
   "id": "7f0d23ef97b795a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
