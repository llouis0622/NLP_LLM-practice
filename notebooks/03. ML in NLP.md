# 1. 텍스트 분류 유형

- 머신러닝 알고리즘을 활용하여 텍스트 내용을 기반으로 미리 정의된 카테고리나 레이블을 할당하는 자연어 처리 작업
- 레이블이 지정된 데이터셋으로 모델을 훈련시켜 새로운 텍스트 입력의 카테고리를 정확하게 예측 가능

## 1. 지도 학습

- 레이블이 부여된 데이터를 통해 학습하여 새롭고 처음 보는 데이터의 레이블을 예측하는 알고리즘
- 각 문서나 텍스트 샘플에 해당 카테고리나 클래스가 레이블로 지정된 데이터셋으로 모델을 훈련시키는 과정
- 각 텍스트 샘플에 해당 카테고리나 클래스가 주석으로 달린 레이블이 지정된 데이터셋 확보
- 레이블이 지정된 데이터셋은 가장 높은 수준의 신뢰성을 가진 것으로 간주
- 모델링을 위해 텍스트 데이터 전처리
- 텍스트 데이터는 단어 가방 또는 TF-IDF 인코딩 같은 기법을 사용하여 수치 특성으로 변환
- 지도 학습 알고리즘이 수치 특성을 사용하여 레이블이 지정된 데이터셋에서 훈련

### 나이브 베이즈

- 베이즈 정리 기반의 확률적 알고리즘
- 관찰된 증거가 주어졌을 때 가설의 확률이 가설이 주어졌을 때 증거의 확률과 가설의 사전 확률의 곱에 비례
- 클래스 레이블이 주어졌을 때 특성들이 서로 독립적이라 가정

### 로지스틱 회귀

- 이진 분류 문제에 사용되는 통계적 방법
- 로지스틱 함수를 사용하여 문서가 특정 클래스에 속할 확률 모델링

### 서포트 벡터 머신

- 데이터를 서로 다른 클래스로 최적의 초평면을 찾는 방식으로 작동
- 특성은 문서의 단어, 초평면은 가능한 모든 문서의 공간을 서로 다른 클래스에 해당하는 여러 영역으로 나누는 것

## 2. 비지도 학습

- 데이터에 레이블이 지정되지 않은 상태에서 알고리즘이 스스로 패턴과 구조를 찾아내는 머신러닝 알고리즘
- 레이블이 없는 데이터를 사용할 때나 텍스트 데이터에서 숨겨진 패턴을 발견하고자 할 때 사용

### 군집화

- 문서의 내용을 바탕으로 유사한 문서들을 그룹화, 각 문서가 무엇에 관한 것인지에 대한 사전 지식 없이도 작동
- 문서 모음에서 주제를 식별하거나 유사한 문서를 묶어 추가 분석을 수행하는 데 사용

### LDA

- 말뭉치의 각 문서가 여러 주제의 혼합으로 구성, 각 주제는 단어들에 대한 확률 분포라는 가정을 하는 확률 생성 모델
- 주제에 대한 명시적 레이블이 없어도 문서 모음에서 숨겨진 주제를 발견하는 데 사용

### 단어 임베딩

- 단어가 나타나는 문맥을 기반으로 의미를 포착하는 밀집 벡터 표현
- 유사한 단어를 식별하고 단어 간의 관계를 찾는 데 유용

### Word2Vec

- 단어를 고차원 공간에서 벡터로 표현하는 단어 임베딩을 생성하는 데 사용
- 유사한 문맥에서 자주 나타나는 단어들이 유사한 의미를 가짐
- 대규모 텍스트 말뭉치를 입력으로 받아 어휘에 있는 각 단어에 대한 벡터 표현 생성
- CBOW : 문맥 단어들을 바탕으로 목표 단어를 예측하려고 시도
- 스킵그램 : 목표 단어를 바탕으로 문맥 단어들을 예측하려고 시도

## 3. 준지도 학습

- 지도 학습과 비지도 학습의 중간에 위치한 머신러닝 패러다임
- 레이블이 있는 데이터와 없는 데이터를 결합하여 모델 훈련
- 레이블이 있는 데이터를 확보하는 데 비용이 많이 들거나 시간이 오래 걸릴 때 유용
- 레이블이 없는 데이터의 정보를 활용하여 분류 작업에서 성능 개선 가능

### 레이블 전파

- 그래프 기반의 준지도 학습 알고리즘
- 레이블이 있는 데이터와 없는 데이터를 모두 사용해 그래프 구성, 각 데이터 포인트는 노드, 노드 간의 유사성은 에지로 표현
- 유사성을 바탕으로 레이블이 있는 노드에서 없는 노드로 레이블을 전파하는 방식으로 작동
- 유사한 데이터 포인터는 유사한 레이블을 가져야 함

### 공동 훈련

- 데이터의 서로 다른 관점에 대해 여러 분류기를 훈련시키는 준지도 학습 방법
- 관점 : 학습 작업에 충분하고 클래스 레이블이 주어졌을 때 조건부로 독립적인 특성들의 부분 집합
- 한 분류기의 예측을 사용하여 일부 레이블이 없는 데이터에 레이블을 지정한 다음, 새롭게 레이블이 지정된 데이터를 사용하여 다른 분류기를 훈련시키는 것

## 4. 원-핫 인코딩 벡터 표현을 이용한 문장 분류

- 단어와 같은 범주형 데이터를 이진 벡터로 나타내는 방법
- 텍스트 데이터를 분류 모델의 수치 입력 특성으로 변환

### 텍스트 전처리

- 원시 텍스트를 머신러닝 알고리즘이 쉽게 이해하고 처리할 수 있는 구조적이고 일관된 형식으로 변환하는 것
- 노이즈 감소 : 전처리를 통해 텍스트를 정리하고 분류 모델 성능에 부정적인 영향을 주는 노이즈 감소
- 차원 축소 : 불용어 제거, 어간 추출, 표제어 추출 등의 전처리로 특성 공간의 차원 감소
- 일관된 표현 : 같은 의미나 어근을 가진 단어들이 원-핫 인코딩 벡터에서 일관되게 표현
- 불필요한 정보 처리 : 분류 작업에 도움이 되지 않는 불필요한 정보 제거
- 모델 성능 향상 : 더 깔끔하고 구조화된 데이터셋을 기반으로 모델이 학습하게끔 함

### 단어 사전 구축

- 전처리된 텍스트에서 모든 고유한 단어를 모아 단어 사전을 구성하고 각 단어에 고유한 인덱스 할당
- 전처리된 텍스트 데이터에 포함된 모든 고유한 단어들의 집합, 각 문서에 대해 원-핫 인코딩 특성 벡터를 생성하는 기반 구축
- 고유한 단어 집합 생성 : 텍스트 데이터를 전처리한 후 모든 문서에서 단어를 수집하여 고유한 단어 집합 생성
- 단어에 인덱스 할당 : 고유한 단어 집합 생성 후 단어 사전의 각 단어에 고유한 인덱스 할당

### 원-핫 인코딩

- 문서의 각 단어에 대해 단어 사전에서 해당하는 인덱스를 찾아 원-핫 인코딩 벡터의 해당 인덱스 값을 1로 설정
- 원-핫 인코딩 벡터를 행으로 하는 특성 행렬 생성

### N-그램

- 단어 가방 모델을 일반화한 것
- 연속된 n개 단어의 순서를 고려하여 단어 배열 반영
- 주어진 텍스트에서 n개의 연속된 항목으로 이루어진 시퀀스

### 모델 훈련

- 머신러닝 모델을 특성 행렬로 훈련시켜 원-핫 인코딩된 텍스트 특성과 목표 레이블 간의 관계 학습
- 문서 내 특정 단어의 유무를 바탕으로 클래스 레이블을 예측하는 방법 학습
- 모델 평가 : 적절한 평가 지표를 사용하여 모델의 성능 평가
- 모델 적용 : 훈련된 모델을 새로운, 보지 않은 텍스트 데이터에 적용

# 2. TF-IDF를 활용한 텍스트 분류

- 문서 모음 내에서 한 문서에 대한 단어의 중요도를 측정하는 데 사용되는 수치 통계
- 문서 내 빈도에 비례하여 증가, 전체 문서 모음에서의 단어 빈도에 의해 상쇄

### TF-IDF 관련 수학적 방정식

- 단어 빈도
    - 문서 d에서 단어 t의 TF는 해당 단어가 문서에 나타나는 횟수를 문서의 총 단어 수로 정규화한 값
    - $TF(t, d) = \frac{\text{단어 t가 문서 d에 나타나는 횟수}}{\text{문서 d의 총 단어 수}}$
    - TF : 특정 문서 내에서 단어의 중요도 측정
- 역문서 빈도
    - 단어 t의 IDF는 전체 문서 모음에서 해당 단어의 희소성을 반영
    - $IDF(t) = \frac{\text{전체 문서 수}}{\text{단어 t를 포함하는 문서 수}}$
    - 단어가 많으 문서에 나타나면 IDF 값은 0에 가까워짐
- TF-IDF 계산
    - 문서 d에서 단어 t의 TF-IDF 값은 문서 내 단어의 TF와 문서 모음 전체에서의 단어 IDF를 곱하여 계산
    - $TF-IDF(t, d) = TF(t, d) \times IDF(t)$
    - 단어가 문서 내에서 자주 등장하고 전체 문서 집합에서 드문 경우 높은 값을 가짐
    - 높은 TF-IDF 값 : 특정 문서에서 중요한 단어
    - 낮은 TF-IDF 값 : 모든 문서에서 흔하게 나타나거나 특정 문서에서 희소한 단어

### 분류기가 수행해야 할 단계

- 텍스트 데이터 전처리 : 모든 문서의 단어 토큰화, 소문자 변환, 불용어 제거, 어간 추출 또는 표제어 추출 적용
- 단어 사전 생성 : 전처리된 문서에서 모든 고유 단어 결합
- TF와 IDF 값 계산 : 각 문서의 각 단어에 대해 TF와 IDF 계산
- TF-IDF 값 계산 : 각 문서의 각 단어에 대해 TF-IDF 값 계산 → 특성 행렬 생성
- 분류기 훈련 : 데이터셋을 훈련 세트와 테스트 세트로 나눔 → 훈련 세트의 TF-IDF 특성 행렬과 해당 레이블을 사용하여 훈련
- 클래스 레이블 예측 : 테스트 세트를 동일한 단어 사전을 사용하여 전처리하고 TF-IDF 값 계산

# 3. Word2Vec을 활용한 텍스트 분류

## 1. Word2Vec

- 단어를 연속적인 벡터 공간에서 밀집 벡터로 표현하는 단어 임베딩을 생성하는 데 사용
- 단어가 텍스트에서 등장하는 문맥을 바탕으로 단어 간의 의미적 관계와 의미 포착

### CBOW

- 주변 문맥 단어들이 주어졌을 때 목표 단어를 예측하는 것
- 문맥 단어 임베딩의 평균을 입력으로 받아 목표 단어 예측
- 문맥 단어들이 주어졌을 때 목표 단어를 관찰할 평균 로그 확률을 최대화
- $\text{목적함수}_{CBOW} = \frac{1}{T}\sum_{\text{대상 단어}}\log(P(\text{문맥}|\text{대상 단어}))$
- $P(\text{문맥}|\text{대상 단어}) = \frac{e^{v^T_{\text{대상 단어}} \cdot v_{\text{문맥}}}}{\sum_i e^{v^T_i \cdot v_{\text{문맥}}}}$

### 스킵그램

- 목표 단어가 주어졌을 때 주변 문맥 단어들을 예측하는 것
- 목표 단어 임베딩을 입력으로 받아 문맥 단어 예측
- 목표 단어가 주어졌을 때 문맥 단어들을 관찰할 평균 로그 확률을 최대화하는 것
- $\text{목적함수}_{Skip-Gram} = \frac{1}{T}\sum_{\text{문맥}}\log(P(\text{문맥}|\text{대상 단어}))$
- $P(\text{문맥}|\text{대상 단어}) = \frac{e^{v^T_{\text{문맥}} \cdot v_{\text{대상 단어}}}}{\sum_i e^{v^T_i \cdot v_{\text{대상 단어}}}}$

## 2. Word2Vec을 활용한 텍스트 분류

- 텍스트 전처리
- Word2Vec 모델 훈련
- 문서 임베딩 생성
- 모델 훈련
- 모델 평가
- 모델 적용

## 3. 모델 평가

### 정확도

- 전체 분류된 기록 중 올바르게 분류된 기록의 수 측정
- $\text{정확도} = \frac{\text{(TP + TN)}}{\text{(TP + TN + FP + FN)}}$

### 정밀도

- 모델이 긍정으로 예측한 총 인스턴스 중에서 올바르게 식별된 긍정 인스턴스의 비율 측정
- $\text{정밀도} = \frac{\text{TP}}{\text{TP + FP}}$

### 재현율

- 진짜 긍정 인스턴스 중에서 올바르게 식별된 긍정 인스턴스의 비율
- $\text{재현율} = \frac{\text{TP}}{\text{TP + FN}}$

### F1 점수

- 정밀도와 재현율의 조화 평균
- $\text{F1 점수} = 2\frac{\text{정밀도} \times \text{재현율}}{\text{정밀도} + \text{재현율}}$

### F1 매크로

- 각 클래스에 대해 개별적으로 F1 점수를 계산한 후 이 값들의 평균을 구함
- 다수 클래스에 더 많은 가중치를 주지 않고 모든 클래스에 걸쳐 분류기의 성능을 평가하고자 할 때 유용
- $F1_{\text{매크로}} = \frac{1}{n}\sum_i F1_i$

### F1 마이크로

- 모든 클래스의 기여도를 집계하여 F1 점수 계산
- 모든 클래스에 걸쳐 전역 정밀도와 재현율 값을 계산한 다음 이 전역 값들을 기반으로 F1 점수 계산
- $F1_{\text{마이크로}} = 2\frac{\text{전역 정밀도} \times \text{전역 재현율}}{\text{전역 정밀도} + \text{전역 재현율}}$
- $\text{전역 정밀도} = \frac{\sum_i\text{TP}}{\sum_i(\text{TP + FP})}$
- $\text{전역 재현율} = \frac{\sum_i\text{TP}}{\sum_i(\text{TP + FN})}$

### 혼돈 행렬

- 분류 모델이 예측한 TP, TN, FP, FN의 개수를 표 형식으로 나타낸 것
- 모델의 효율성에 대한 세부적인 시각 제공 → 모델의 강점과 약점 파악 가능

# 4. 토픽 모델링 : 비지도 텍스트 분류의 특정 사례

- 대규모 문서 집합 내에서 추상적인 주제나 테마를 발견하는 데 사용되는 비지도 머신러닝 기법
- 각 문서를 여러 주제의 혼합으로 표현할 수 있고 각 주제는 단어에 대한 분포로 표현될 수 있다고 가정
- 문서 내에서 주제와 그에 해당하는 단어 분포를 찾고 각 문서의 주제 비율을 구하는 것

## 1. LDA

- 생성적 확률 모델
- 관찰된 문서를 가장 잘 설명하는 주제-단어 분포($\varphi$)와 문서-주제 분포($\theta$)를 찾는 것

### LDA 생성 과정

- 문서의 단어 수 선택
- 디리클레 분포로부터 매개변수 $\alpha$를 사용해 문서의 주제 분포 $\theta$ 선택
- 문서 내에서 각 단어에 대해 다음을 수행
    - 주제 분포 $\theta$로부터 주제 $z$ 선택
    - 선택된 주제의 단어 분포 $\varphi$로부터 단어 $w$ 선택. $\varphi$는 $\beta$ 매개변수를 사용한 디리클레 분포에서 생성된 해당 주제에 대한 단어 분포

### LDA 수식 표현

- $P(z, w \mid \theta, \phi) = \prod_{i = 1}^M \prod_{j = 1}^N P(w_{ij} \mid \phi, z_{ij})P(z_{ij} \mid \theta_i)$
- $P(w \mid \alpha, \beta) = \int\int P(w \mid \theta, \phi)P(\theta \mid \alpha)P(\phi \mid \beta)d \theta d \phi$

### LDA 수행 과정

- 전처리
- 초기화 : 디리클레 사전 분포 $\alpha$와 $\beta$ 초기 값 선택
- 무작위 주제 할당 : 각 문서의 각 단어에 무작위로 주제 할당
- 반복적 추론
    - 주제 할당과 주제-단어 분포 및 문서-주제 분포를 반복적으로 업데이트하여 수렴하거나 일정한 횟수의 반복에 도달할 때까지 수행
    - 기브스 샘플링, 변분 추론
- 해석 : 각 주제에 대해 가장 확률이 높은 단어와 각 문서에 대해 가장 확률이 높은 주제를 확인하여 해석

## 2. 자연어 처리 텍스트 분류를 위한 실제 머신러닝 시스템 설계 및 솔루션 구현

- 비즈니스 목표 수립
- 기술적 목표 수립
- 고수준 시스템 설계 초안
- 평가 지표 선택
- 탐색 : 데이터 탐색 & 실현 가능성 연구
- 결과 평가
- 배포
- 코드 설계