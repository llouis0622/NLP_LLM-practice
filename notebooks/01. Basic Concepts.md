# 1. 데이터 탐색

## 1. 데이터 시각화

- 차트, 그래프 등 다양한 시각적 도구를 사용하여 데이터를 표현
- 텍스트 데이터의 언어적 패턴과 구조 이해 가능
- 산점도 : 두 변수 간의 관계 표현(패턴/트렌드 식별)
- 히스토그램 : 단일 변수의 분포 표현(이상치/비정상적인 데이터 감지, 편향된 영역 식별)
- PCA, t-SNE 차원 축소 기법 + 네트워크 시각화

## 2. 데이터 정제

- 데이터셋 내의 오류, 불일치, 부정확성 식별 후 수정 및 제거

### 결측치 다루기

- 행 제거하기 : 결측치를 포함하는 행 삭제(결측치 행이 적을 경우)
- 열 제거하기  : 결측치를 포함하는 열 삭제(결측치 열이 적을 경우)
- 평균/중앙값/최빈값 대체 : 결측치를 비결측치에서 계산한 평균/중앙값/최빈값으로 대체
- 회귀 대체 : 데이터셋의 다른 변수들의 값을 기반으로 결측치 예측
- 다중 대체 : 통계 모델을 사용해 여러 개의 대체된 데이터셋 생성 후 결과 종합
- k-최근접 이웃 대체 : 결측치에 가장 가까운 K개의 데이터 포인트 식별 후 이 값을 활용하여 대체

### 중복 제거

- 데이터셋의 모든 행을 비교하여 중복 레코드 식별
- 고유 식별자 열
- 중복 레코드의 첫 번째 발생만 유지하고 이후 발생 모두 제거
- 가장 완전한 정보를 가진 레코드나 가장 최근의 타임스탬프를 가진 레코드 유지

### 데이터 표준화와 변환

- 표준화(Z-점수 정규화) : 각 특성을 평균이 0, 표준 편차가 1이 되도록 변환
    - $x' = \frac{x - \text{mean}(x)}{\text{std}(x)}$
    - 각 특성의 범위가 0을 중심으로 조정 → 큰 값을 가진 특성의 과도한 영향력 방지 가능
- 최소-최대 스케일링 : 데이터를 일관된 값 범위로 재조정(0~1)
    - $x = \frac{x - \min(x)}{\max(x) - \min(x)}$
    - 데이터의 정확한 분포 보다 다른 특성들 간의 의미 있는 비교를 위해 데이터 표준화
- 로그 변환 : 데이터의 이상치와 왜도의 영향 완화

### 이상치 처리

- 이상치 제거 : 이상치로 식별된 관측치 데이터셋에서 제거
- 데이터 변환 : 로그, 제곱근 등의 수학적 함수를 적용하여 데이터 변환
- 윈저화 : 극단값을 데이터셋에서 가장 높은(낮은) 값으로 대체
- 값 대체 : 결측값 또는 극단값을 데이터셋의 나머지 관측치에서 추정된 값으로 대체
- 강건한 통계 기법 사용 : 이상치에 대한 민감도가 낮아 극단값이 존재하더라도 정확한 결과 제공 가능

### 오류 수정

- 수동 검사 : 데이터셋을 수동으로 검사하여 오류 직접 수정
- 통계적 방법 : 데이터가 특정 분포를 따를 때 통계적 기법을 사용하여 이상치 감지 후 제거 및 대체
- 머신러닝 방법 : 머신러닝 알고리즘 적용
- 도메인 지식 : 데이터 내 예상 범위를 고려하여 오류 식별 및 수정
- 값 대체 : 데이터의 결측값 채우기

## 3. 특성 선택

### 필터 방법

- 카이제곱
    - 두 무작위 변수 간의 의존성 측정
    - 실제 관측치와 동일한 정도로 극단적이거나 더 극한적인 결과를 얻을 가능성을 나타내는 P-값 제공
    - 가설 검정 : 수집된 데이터가 예상 데이터와 일치하는지 평가
    - 특성 선택 : 데이터셋의 각 특성과 목표 변수 간의 관계 평가
    - $X^2 = \sum\frac{(O_i - E_i)^2}{E_i}$
- 상호 정보량
    - 두 무작위 변수 간의 상호 의존성을 측정하는 지표
    - 각 특성과 목표 변수 사이의 상호 정보량을 계산하고 최종적으로 가장 높은 상호 정보량 점수를 가진 특성들을 선택
    - $I(X;Y) = \sum_{x \in X}\sum_{y \in Y} P(x, y)\log(\frac{P(x, y)}{P(x)P(y)})$
- 상관 계수
    - 두 변수 간의 선형 관계의 강도와 방향을 나타내는 지표
    - 피어슨 상관 계수 : 두 연속형 변수 간의 선형 관계 측정(-1~1)
        
        $r = \frac{\text{cov}(X, Y)}{\text{std}(X)\cdot\text{std}(Y)}$
        
    - 비선형/범주형 → 스피어먼 순위 상관 계수/켄달 상관 계수

### 래퍼 방법

- 반복적인 모델 훈련과 테스트를 통해 특성 부분 집합을 탐색하는 기술
- 재귀적 특성 제거
    - 사전 설정된 특성 수가 남을 때까지 중요도가 낮은 특성을 하나씩 제거
    - 후진 제거 접근법으로 작동
- 전진 선택/후진 제거

### 임베디드 방법

- 모델 훈련 과정 중에 특성을 선택하는 기법
- 라쏘 회귀
    - 최소 절대 수축 및 선택 연산자의 약자
    - 표준 회귀 손실 함수에 패널티 항을 도입하는 방법
    - 특성의 수가 샘플 수를 크게 상회하는 고차원 데이터에 유용
    - $\min_w ||y - Xw||_2^2 + \lambda ||w||_1$
- 릿지 회귀
    - 특성 선택에 적용 가능한 선형 회귀 방법
    - 일반적인 최소 제곱 회귀와 유사하지만 비용 함수에 패널티 항 추가
    - 계수 크기의 제곱에 비례하는 패널티 항을 포함하여 비용 함수 수정
    - $\min_w ||y - Xw||_2^2 + \alpha ||w||_2^2$
- 데이터셋에 많은 특성이 있고 그중 일부만 중요할 것으로 예상되는 경우 → 라쏘 회귀
- 대부분의 특성이 어느 정도 관련성이 있을 것으로 예상되는 경우 → 릿지 회귀

### 차원 축소

- 가능한 한 많은 정보를 유지하면서 특성을 저차원 공간으로 변환하는 방법
- PCA
    - 상관된 변수 집합을 주성분이라는 상관되지 않은 변수 집합으로 변환
    - 데이터에서 최대 분산의 방향을 식별하고 데이터를 이러한 방향으로 투영하여 데이터의 차원을 축소
    - 데이터 표준화 : 각 특성은 평균이 0, 분산이 1
    - 공분산 행렬 계산 : 데이터의 특성 쌍 간의 선형 관계를 측정하는 정방행렬
    - 공분산 행렬의 고유벡터와 고윳값 계산 : 고유벡터는 데이터셋 내에서 가장 높은 분산의 주요 방향을 나타내고 고윳값은 각 고유벡터가 설명하는 분산의 크기를 나타냄
    - 주성분의 수 선택 : 고윳값을 분석하여 가장 많은 분산을 설명하는 상위 k개의 고유벡터를 선택하여 유지할 주성분의 수 결정
    - 선택된 주성분으로 데이터 투영 : 원래 데이터를 선택된 주성분에 투영하여 저차원으로 표현
- LDA
    - 원래의 특성을 낮은 차원의 공간으로 변환하여 클래스 간 차별적 정보를 최대한 유지하면서 특성 수 감소
    - 원래의 특성들의 선형 결합을 찾아 클래스 간의 분리 극대화
    - 각 클래스의 평균 벡터 계산
    - 각 클래스의 공분산 행렬 계산
    - 전체 평균 벡터와 전체 공분산 행렬 계산
    - 클래스 간 산포 행렬 계산
    - 클래스 내 산포 행렬 계산
    - $S_w^{-1}S_b$로 행렬의 고유벡터와 고윳값 계산
    - 가장 높은 고윳값을 갖는 상위 k개 고유벡터를 새로운 특성 공간으로 선택
- t-SNE
    - 데이터 포인트 간의 거리를 보존하는 대신, 저차원 공간에서 데이터 포인트 쌍 간의 유사성 보존
    - 고차원 공간에서 각 데이터 포인트 쌍 간의 유사성 계산
    - 유사성 행렬을 소프트맥스 함수를 사용하여 확률 분포 변환
    - 저차원 공간에서 각 데이터 포인트 쌍 간의 유사성 계산
    - 고차원 공간의 쌍별 유사성과 저차원 공간의 쌍별 유사성 간의 차이 최소화를 위해 저차원 공간에서 점들의 위치 조정

## 4. 특성 공학

- 특성 선택 : 모델의 정확도를 높이고 복잡성을 줄이기 위해 원래 데이터셋에서 관련 있는 특성의 하위 집합을 선택
- 특성 추출 : 원시 데이터를 모델에 더 유용한 새로운 특성 집합으로 변환
- 특성 스케일링 : 특성값을 동일한 범위로 조정
- 특성 생성 : 기존의 특성을 결합하거나 변환하여 새로운 특성을 만드는 과정

### 특성 스케일링

- 최소-최대 스케일링
    - 특성의 값을 특정 범위(0~1)로 조정
    - $x_{scaled} = \frac{x - \min(x)}{\max(x) - \min(x)}$
- 표준화
    - 특성값들을 평균이 0, 표준 편차가 1이 되도록 변환
    - $x_{scaled} = \frac{x - \text{mean}(x)}{\text{std}(x)}$
- 로버스트 스케일링
    - 표준화와 유사하지만 평균과 표준 편차 대신 중앙값과 사분위수 범위 사용
    - $x_{scaled} = \frac{x - \text{median}(x)}{\text{Q3}(x) - \text{Q1}(x)}$
- 로그 변환
    - 데이터가 매우 치우쳐 있거나 긴 꼬리를 가질 때 특성값의 로그를 취함
    - $x_{transformed} = \log(x)$
- 거듭제곱 변환
    - 로그 변환과 유사하지만 더 넓은 범위의 변환 가능
    - Box-Cox 변환 - 최대 우도 추첮응 ㄹ사용하여 결정된 거듭제곱으로 특성값을 올림
    - $x_{transformed} = \frac{x^\lambda - 1}{\lambda}$

### 다항식 확장

- 기존 특성의 다항식 조합을 통해 새로운 특성을 생성하는 특성 생성 기법
- 특성과 목표 변수 간의 비선형 관계 모델링
- 기존 특성들을 다양한 차수로 거듭제곱하고 이들의 곱을 취해 새로운 특성 생성

### 로그 변환

- 특성에 로그 함수를 적용하여 데이터를 덜 왜곡되고 더 대칭적으로 만드는 것
- 데이터의 자연 로그를 취하는 방식
- $y = \log(x)$
- 데이터를 정규화하여 정규 분포를 가정하는 특정 머신러닝 알고리즘에 더 적합하게 만드는 것
- 데이터의 이상치 영향을 줄여 일부 모델의 성능 향상

### 상호작용 항

- 데이터셋의 두 개 이상의 기존 특성을 수학적 연산을 통해 결합하여 새로운 특성을 생성하는 것
- 새로운 특성들로 원래 특성들 간의 상호작용이나 관계 포착 가능
- 도메인 지식 활용 : 해당 분야의 전문 지식이나 직관을 활용하여 어떤 특성들이 상호작용할 가능성이 있는지 파악
- 쌍별 조합 : 데이터셋의 모든 가능한 특성 쌍을 조합하여 상호작용 항 생성
- PCA 활용 : 주성분 분석을 사용하여 가장 중요한 특성 조합 식별

# 2. 일반적인 머신러닝 모델

## 1. 선형 회귀

- 종속 변수와 하나 이상의 독립 변수 간의 관계를 모델링하는 지도 학습 알고리즘
- 독립 변수를 기반으로 종속 변수의 값을 예측하는 최적의 직선을 찾는 것
- $y = mx + b$
- $y = b_0 + b_1x_1 + b_2x_2 + \cdots + b_nx_n$

### 선형 회귀 장점

- 단순하고 이해하기 쉬움
- 종속 변수와 독립 변수 간의 다양한 관계 모델링 가능
- 계산 효율이 높아 속도가 빠르고 대규모 데이터셋에도 적합
- 해석 가능한 결과를 제공하여 각 독립 변수가 종속 변수에 미치는 영향 분석 가능

### 선형 회귀 단점

- 입력 특성과 출력 사이에 선형 관계가 있다고 가정, 실제 데이터에서 항상 성립하지 않을 수 있음
- 입력 특성과 출력 사이의 복잡한 비선형 관계 포착 불가
- 이상치와 영향력 있는 관측치에 민감하여 모델 정확도에 영향
- 오차가 일정한 분산을 가진 정규 분포를 따른다고 가정하지만, 실제로 그렇지 않을 수 있음

## 2. 로지스틱 회귀

- 이진 결과와 같은 이산적인 결과를 예측하는 데 활용
- 하나 이상의 입력 변수를 기반으로 특정 결과가 발생할 확률을 추정하는 것
- 입력 변수와 출력 변수 간의 관계가 로짓 공간에서 선형적이라 가정
- $logit(p) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_nx_n$

### 로지스틱 회귀 장점

- 해석 가능성 : 각 입력 변수가 예측된 긍정적 결과 확률에 어떻게 영향을 주는지 쉽게 이해 가능
- 계산 효율성 : 간단한 알고리즘으로 대규모 데이터셋에서도 빠르게 학습 가능
- 소규모 데이터셋에서의 효과성 - 적은 수의 관측치로도 효과적인 작동 가능

### 로지스틱 회귀 단점

- 선형성 가정 : 입력 변수와 긍정적 결과 확률의 로짓 사이에 선형 관계가 있다고 가정, 실제 데이터셋에서는 항상 이 가정이 성립하지 않을 수 있음
- 과대적합 가능성 : 입력 변수의 수가 관찰값의 수보다 많을 경우 모델이 과대적합되어 새로운 데이터에 대해 일반화 성능이 저하될 위험 존재
- 비선형 문제에 부적합 : 입력 변수와 출력 변수 간의 관계가 비선형적 문제에는 부적합

## 3. 결정 트리

- 트리 : 결정 지점을 나타내는 일련의 노드로 구성
- 노드 : 하나 이상의 가지를 통해 다른 결정 지점이나 최종 예측으로 연결
- 분류 문제 : 트리의 각 잎 노드가 특정 클래스 표현
- 회귀 문제 : 각 잎 노드가 구체적인 수치값을 표현
- 목표 변수에 관해 더욱 동질적인 부분집합으로 데이터를 가장 효과적으로 분할하는 속성 순서를 선별
- 각 결정 지점에서 가능한 모든 분할에 대해 정보 이득을 계산하는 것
- 엔트로피 = $\sum_{i=1}^c -p_i \log_2 p_i$

### 결정 트리 장점

- 쉽게 이해하고 해석할 수 있는 직관적인 모델
- 범주형 데이터와 수치형 데이터를 모두 효과적으로 처리 가능
- 결측치와 이상치 다루기 가능
- 중요한 특성을 선별하는 데 활용 가능
- 앙상블 방법에서 다른 모델과 결합하여 성능 향상 가능

### 결정 트리 단점

- 트리가 너무 깊거나 복잡한 경우 과대적합에 취약
- 데이터의 작은 변화나 트리 구축 방식에 민감
- 많은 범주나 높은 카디널리티를 가진 특성에 편향
- 발생 빈도가 낮은 이벤트나 불균형한 데이터셋을 다루는 데 한계 존재

## 4. 랜덤 포레스트

- 여러 개의 결정 트리 생성
- 분류 : 다수의 트리가 예측한 클래스가 최종 결과
- 회귀 : 트리들이 예측한 값의 평균이 최종 결과
- $\hat{y} = \argmax_j\sum_i I(y_{i, j} = 1)$
- $\hat{y} = \frac{1}{T}\sum_{i=1}^T y_i$

### 랜덤 포레스트 알고리즘

- 부트스트랩 샘플링 : 데이터를 무작위로 복원 추출하여 원본 데이터셋과 같은 크기의 새로운 데이터셋 생성
- 특성 선택 : 결정 트리를 만들 때 분할마다 무작위로 선택된 특성들의 부분 집합 사용
- 트리 구축 : 부트스트랩 샘플과 특성의 부분 집합을 사용해 결정 트리 생성
- 앙상블 학습 : 모든 결정 트리의 예측을 결합하여 최종 예측 도출

### 랜덤 포레스트 장점

- 견고성 : 수치형, 범주형, 순서형 데이터를 포함한 다양한 입력 데이터 유형 추리 가능
- 특성 선택 : 특성의 중요도 순위화 가능, 사용자가 분류나 회귀 작업에 가장 중요한 특성 식별 가능
- 과대적합 방지 : 배깅(과대적합 감소 메커니즘) 내장으로 새로운 데이터에 대해 일반화 좋음
- 확장성 : 다양한 특성을 가진 대규모 데이터셋 처리로 빅데이터 응용에 적합
- 이상치 처리 : 결정 트리 기반으로 이상치 효과적 처리 가능

### 랜덤 포레스트 단점

- 해석 가능성 : 결정 트리 앙상블 기반으로 해석하기 어려움
- 훈련 시간 : 다른 단순한 알고리즘보다 훈련 시간이 김
- 메모리 사용 : 결정 트리를 메모리에 저장, 다른 알고리즘보다 더 많은 메모리 필요
- 편향 : 데이터가 불균형하거나 목표 변수의 카디널리티가 높은 경우 편향 존재
- 과대적합 : 하이퍼파라미터가 제대로 조정되지 않으면 여전히 과대적합 문제 발생

## 5. 서포트 벡터 머신(SVM)

- 복잡합 결정 경계가 필요한 상황에서 뛰어난 성능 발휘
- 다차원 공간에서 클래스들을 최대한 분리할 수 있는 초평면을 찾는 것
- $y = sign(w^Tx + b)$
- 마진을 최대화하는 조건 하에서 분류 오류를 최소화하는 것

### 서포트 벡터 머신 장점

- 고차원 공간에서 효과적, 특성 수가 많을 때 유용
- 분류와 회귀 작업 모두 사용 가능
- 선형/비선형 분리 데이터 모두 작동 가능
- 마진 개념 적용으로 이상치에 강함
- 과대적합을 제어할 수 있는 정규화 매개변수 보유

### 서포트 벡터 머신 단점

- 커널 함수 선택에 민감, 커널 함수 선택에 따라 모델 성능이 크게 달라짐
- 대규모 데이터셋에서는 계산 비용 큼
- 서포트 벡터 머신 모델의 결과 해석 어려움
- 좋은 성능을 얻기 위해 매개변수 조정 필요

## 6. 인공 신경망과 트랜스포머

### 신경망

- 다수의 상호 연결된 노드로 구성된 여러 층으로 이루어짐
- 데이터를 효율적으로 처리하고 변환
- $y = f(\sum_{i=1}^nw_ix_i + b)$
- 손실 함수를 최소화하도록 뉴런의 가중치와 편향을 조정하는 과정
- 입력과 출력 데이터 간의 복잡한 비선형 관계를 학습할 수 있는 능력
- 원시 데이터에서 의미 있는 특성을 자동으로 추출할 수 있는 능력
- 대규모 데이터셋에 대한 확장성
- 높은 계산 및 메모리 요구 사항
- 하이퍼파라미터 튜닝에 대한 민감성
- 내부 표현을 해석하기 어려움

### 트랜스포머

- 텍스트나 음성처럼 순차적인 데이터를 처리하는 데 적합한 신경망 아키텍처의 일종
- 셀프 어텐션 메커니즘 : 모델 출력 계산 시 입력 시퀀스의 다양한 부분에 주의 집중 가능
- 쿼리 벡터, 키 벡터, 값 벡터 간의 내적 기반
- 값 벡터에 가중치 부여 후 결합하여 최종 출력 생성
- $Q = XW_Q, K = XW_K, V = XW_V, A(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_K}})V$
- 가변 길이 입력 시퀀스를 처리할 수 있는 능력
- 데이터의 장거리 의존성을 포착할 수 있는 능력
- 많은 자연어 처리 작업에서 최첨단 성능
- 높은 계산 및 메모리 요구 사항
- 하이퍼파라미터 튜닝에 대한 민감성
- 순차적 동적 모델링이 명시적으로 필요한 작업 처리 어려움

# 3. 모델 과소적합과 과대적합

## 1. 과소적합

- 모델이 너무 단순해서 데이터의 복잡한 패턴을 제대로 학습하지 못하는 상황
- 모델이 충분히 훈련되지 않았거나 모델의 복잡성이 데이터의 기본 패턴을 포착하기에 부족할 때 발생

## 2. 최적 적합

- 모델이 데이터의 패턴을 잘 포착하면서도 모든 샘플에 과도하게 맞추지 않는 상태
- 새로운 데이터에 대해 더 나은 성능 발휘 가능

## 3. 과대적합

- 너무 복잡해져서 훈련 데이터에 지나치게 맞춰질 때 발생
- 새로운 데이터에 대한 일반화 성능 감소
- 모델이 데이터의 본직적인 패턴 대신 훈련 데이터의 노이즈나 우연한 변동 학습 시 발생

## 4. 편항과 분산

- 편향-분산 트레이드오프 : 편향과 분산 사이의 균형을 맞추기 위해 너무 단순하지도 복잡하지도 않은 모델을 선택하는 것
- 편향 : 모델의 예측값과 훈련 데이터의 실젯값 사이의 차이
    
    실제 세계의 문제를 단순화된 모델로 근사할 때 발생하는 오류
    
- 분산 : 훈련 데이터의 작은 변화에 대한 모델의 민감도
    
    모델이 훈련 데이터의 작은 변동에 민감하게 반응하여 발생하는 오류
    
- 높은 편항 + 낮은 분산 → 모델 복잡도를 높여 개선
- 높은 분산 + 낮은 편향 → 모델 복자도를 낮춰 개선
- 정규화 : 손실 함수에 패널티 항을 추가하여 모델 복잡도 제어
- 앙상블 : 여러 모델을 결합하여 분산을 줄임으로써 전체 성능 향상
- 교차 검증 : 모델의 성능 평가 및 하이퍼파라미터 조정으로 편향과 분산 사이의 최적 균형 확인

## 5. 과대적합 방지

### 교차 검증

- 데이터의 일부로 모델 훈련, 나머지로 테스트
- 서로 다른 부분집합을 훈련과 평가에 번갈아 사용 → 과대적합의 위험 감소

### 정규화

- 훈련 과정에서 손실 함수에 패널티 항을 추가하여 모델의 복잡도를 줄이고 과대적합 방지
- L1 정규화(라쏘), L2 정규화(릿지), 엘라스틱넷 정규화 등

### 조기 종료

- 검증 데이터에 대한 모델의 성능이 저하되기 시작할 때 훈련 과정을 중단하는 기법
- 모델이 이미 최대 성능에 도달했을 때 훈련 데이터로부터 계속 학습하는 것을 막아 과대적합을 방지

### 드롭아웃

- 훈련 과정 중 일부 뉴런을 무작위로 비활성화
- 모델이 특정 특성이나 뉴런에 과도하게 의존하는 것 방지
- 훈련 데이터에 과대적합되는 것 방지

### 데이터 증강

- 기존 데이터셋에 회전, 크기 조정, 좌우 반전 등의 변형을 적용하여 인위적으로 훈련 데이터 양을 늘리는 방법
- 다양한 학습 예제 제공 → 과대적합 완화 및 모델의 일반화 능력 향상

### 앙상블 방법

- 여러 개의 모델을 결합하여 전체적인 성능을 개선하고 과대적합을 방지하는 기법
- 배깅, 부스팅, 스태킹 등의 다양한 앙상블 기법

# 4. 데이터 분할

## 1. 데이터 분할

- 훈련 세트 : 모델 훈련 시 사용
- 검증 세트 : 가장 좋은 성능을 제공하는 하이퍼파라미터 선택 시 사용
- 테스트 세트 : 모델 성능 평가 시 사용

## 2. 분할 방법

- 훈련-테스트 분할 : 무작위 두 세트로 분할(80:20)
- 훈련-검증-테스트 분할 : 세 세트로 분할(60:20:20)
- k-폴드 교차 검증
    - 데이터를 k개의 동일한 크기의 폴드로 나누고, 모델을 k번 반복해서 훈련 및 테스트
    - 각 반복에서 하나의 폴드가 테스트 세트, 나머지가 훈련 세트
    - 데이터세트가 작아 단순 훈련-테스트 분할 시 성능 평가에 큰 편차 발생 시 유용
- 계층화 K-폴드 교차 검증
    - 모든 폴드에서 목표 변수의 분포가 일정하게 유지되도록 함
    - 한 클래스의 샘플 수가 다른 클래스에 비해 현저히 적은 불균형 데이터셋 사용 시 유용
- 시계열 교차 검증
    - 데이터의 시간 순서 유지
    - 데이터를 여러 구간으로 나누고 각 구간이 일정한 시간 간격을 나타내도록 함
    - 과거 데이터를 학습하고 미래 데이터를 테스트하는 방식

# 5. 하이퍼파라미터 튜닝

- 주어진 모델에 가장 적합한 하이퍼파라미터 세트를 선택하는 과정
- 학습률, 정규화 강도, 신경망의 은닉층 수 등
- 모델의 최적 성능을 이끌어내는 최상의 하이퍼파라미터 조합을 찾는 것

## 1. 하이퍼파라미터 튜닝 방법

### 그리드 탐색

- 가능한 모든 하이퍼파라미터 조합을 격자 형태로 만들어 각각을 평가
- 이전 결과를 고려하지 않고 탐색 공간을 전체적으로 또는 무작위로 탐색

### 랜덤 탐색

- 정해진 분포에서 하이퍼파라미터를 무작위로 추출하여 평가
- 이전 결과를 고려하지 않고 탐색 공간을 전체적으로 또는 무작위로 탐색

### 베이지안 최적화

- 함수의 사후 분포를 반복적으로 계산하고 과거 평가 결과를 고려하여 최적의 하이퍼파라미터 찾기
- 과거의 평가 결과를 바탕으로 하이퍼파라미터를 목적 함수의 점수와 확률적으로 매핑
- 베이지안 최적화 과정
    - 목적 함수에 대한 대리 확률 모델 구축
    - 대리 모델을 기반으로 최적의 하이퍼파라미터 추정
    - 추정된 하이퍼파라미터를 실제 목적 함수에 적용
    - 새로운 결과를 반영하여 대리 모델 업데이트
    - 최대 반복 횟수나 시간 제한에 도달할 때까지 반복

### 순서 모델 기반 최적화

- 각 단계에서 더 나은 하이퍼파라미터를 시도하고 그 결과를 바탕으로 확률 모델을 업데이트 하는 방식
- 가우스 프로세스, 랜덤 포레스트 회귀, 트리 구조 파젠 추정기 등의 다양한 기법

### 탐색 공간 감소 표준 기법

- 통계적 샘플링을 통해 훈련 데이터셋 감소
- 특징 선택 기법으로 각 시도의 실행 시간 단축
- 최적화 대상이 되는 주요 하이퍼파라미터 식별
- 정확도, 작업 수, 최적화 시간 등의 추가적인 목표 함수 사용
- 역합성곱 네트워크

# 6. 앙상블 모델

## 1. 배깅

- 훈련 데이터의 서로 다른 부분집합에서 학습된 여러 독립적인 모델을 결합하는 앙상블 방법
- 모델의 분산 감소 및 일반화 성능 개선
- 기본 모델이 불안정한 경우와 훈련 데이터셋이 비교적 작은 경우 효과적
- $Y_{ensemble} = \argmax_j \sum_{i=1}^m I(y_{ij} = j)$
- $Y_{ensemble} = \frac{1}{m}\sum_{i=1}^m y_i$

### 배깅 알고리즘 단계

- 크기 n의 훈련 데이터셋에 대해 m개의 크기 n 부트스트랩 샘플 생성
- 각 부트스트랩 샘플에 대해 독립적으로 기본 모델 학습
- 모든 기본 모델의 예측을 집계하여 최종 앙상블 예측 획득

### 배깅 장점

- 분산 감소와 과대적합 방지로 모델의 일반화 성능 향상
- 복잡한 관계를 가진 고차원 데이터셋 효과적 처리 가능
- 다양한 유형의 기본 모델과 함께 사용 가능

### 배깅 단점

- 여러 개의 기본 모델을 사용하는 만큼 모델 복잡성과 계산 시간 증가
- 기본 모델이 지나치게 복잡하거나 데이터셋이 너무 작은 경우 과대적합 발생
- 기본 모델들이 상관관계가 매우 높거나 편향된 경우 배깅의 효과 감소

## 2. 부스팅

- 약한 분류기들의 성능을 결합해 더 강력한 분류기를 만드는 앙상블 학습 기법
- 훈련 예제의 가중치를 조정하여 분류기의 정확도를 반복적으로 개선
- 이전 약한 분류기의 실수를 학습하고 이전 반복에서 잘못 분류된 예제에 더 큰 가중치 부여

### AdaBoost 알고리즘

- 훈련 예제의 가중치를 모두 동일하게 초기화
- 훈련 세트에서 약한 분류기 학습
- 약한 분류기의 가중 오류율 계산
- 약한 분류기의 가중 오류율을 바탕으로 해당 분류기의 중요도 계산
- 약한 분류기가 잘못 분류한 예제들의 가중치 증가
- 예제들의 가중치가 합해서 1이 되도록 정규화
- 미리 정해진 반복 횟수 또는 원하는 정확도에 도달할 때까지 반복
- 약한 분류기들을 중요도에 따라 가중치를 부여하여 강력한 분류기로 결합

### 부스팅 수식

- 가중 오류율
    
    $\text{가중 오류율}_m = \frac{\sum_{i=1}^N w_i I(y_i - h_m(x_i))}{\sum_{i=1}^N w_i}$
    
- 약한 분류기 중요도
    
    $\alpha_m = \ln(\frac{1 - \text{오류}_m}{\text{오류}_m})$
    
- 훈련 예제 가중치
    
    $w_i = w_i^{\alpha_m I(y_i - h_m(x_i))}$
    
- 최종 분류기
    
    $H(x) = sign(\sum_{m=1}^M \alpha_mh_m(x))$
    

### 부스팅 장점

- 약한 분류기의 성능을 크게 개선하여 전체적인 예측 정확도 향상
- 구현이 비교적 간단, 다양한 유형의 분류 문제에 적용 가능
- 노이즈가 있는 데이터에 대해서도 견고한 성능

### 부스팅 단점

- 이상치에 민감, 노이즈가 많은 데이터에 대해 과대적합 가능성 존재
- 대규모 데이터셋을 다룰 때 상당한 계산 리소스 필요
- 여러 약한 분류기를 복잡하게 조합 → 최종 모델의 결정 과정 해석 어려움

## 3. 스태킹

- 기본 모델의 예측을 결합하여 더 나은 성능을 얻기 위해 상위 모델을 학습시키는 앙상블 학습 기법
- 다양한 기본 모델의 강점을 결합해 더 우수한 예측 성능 달성

### 스태킹 작동 방식

- 훈련 데이터를 두 부분으로 분할
    - 기본 모델들을 학습시키는 데 사용
    - 기본 모델들이 예측한 결과로 새로운 데이터셋을 만드는 데 사용
- 첫 번째 부분의 훈련 데이터를 사용해 여러 기본 모델 학습
- 훈련된 기본 모델들을 사용해 두 번째 부분의 훈련 데이터에 대해 예측 수행, 예측 결과로 새로운 데이터셋 생성
- 만들어진 예측 데이터셋을 활용해 상위 모델 학습
- 훈련된 상위 모델을 사용해 테스트 데이터에 대한 최종 예측 수행

## 4. 랜덤 포레스트

- 여러 결정 트리의 예측을 결합하여 최종 예측 도출
- 일반적으로 단일 모델보다 더 높은 정확도 및 과대적합에 대한 저항력 강함

## 5. 그레이디언트 부스팅

- 약한 분류기를 기반으로 각 단계에서 약한 분류기를 점진적으로 개선하여 더 강력한 모델을 구축하는 방식
- 모델이 이전 단계에서의 오류에 집중, 그 오류를 수정하는 방향으로 모델 개선

### 그레이디언트 부스팅 과정

- 각 반복 단계에서 알고리즘은 예측값에 대한 손실 함수의 음의 그레이디언트를 계산한 후 이 음의 그레이디언트 값을 예측하는 결정 트리 학습
- 새로운 트리의 예측 결과는 이전 트리들의 예측과 결합 → 학습률 파라미터를 통해 각 트리가 최종 예측에 기여하는 정도 조절
- 각 트리의 예측을 학습률로 가중하여 합산한 값

### 그레이디언트 부스팅 수식

- 상수값 초기화
    
    $F_0(x) = \argmin_c \sum_{i=1}^N L(y_i, c)$
    
- 모델 예측 업데이트
    
    $F_m(x) = F_{m-1}(x) + \eta h_m(x)$
    
- 모델 최종 예측
    
    $F(x) = \sum_{m=1}^M \eta_m h_m(x)$
    

### 그레이디언트 부스팅 장점

- 높은 예측 정확도
- 회귀와 분류 문제 모두 처리 가능
- 결측값과 이상치 모두 처리 가능
- 다양한 손실 함수와 함께 사용 가능
- 고차원 데이터도 효과적으로 처리 가능

### 그레이디언트 부스팅 단점

- 과대적합에 민감, 트리의 개수가 많을 때 위험 증가
- 대규모 데이터셋의 경우 학습 과정이 계산적으로 비용이 많이 들고 시간이 오래 걸림
- 트리의 수, 학습률, 트리의 최대 깊이 등 하이퍼파라미터 신중한 조정 필요

# 7. 불균형 데이터 다루기

- 언더샘플링
    - 다수 클래스에서 훈련 데이터 감소
    - 훈련 데이터 감소 시 모델에 전달되는 정보 감소로 훈련 과정과 최종 모델 안정성 감소
- 리샘플링
    - 원래 데이터셋을 수정하여 클래스 간 균형을 맞추는 것
    - 소수 클래스 → 오버샘플링 : 랜덤 오버샘플링, 합성 소수자 오버샘플링, 적응형 합성 샘플링
    - 다수 클래스 → 언더샘플링 : 랜덤 언더샘플링, 토멕 링크, 클러스터 중심점
- 머신러닝 모델에서 불균형 데이터셋 처리 : 비용 함수 수정이나 딥러닝 모델에서 배치 방법 수정 등의 방법

## 1. SMOTE

- 소수 클래스의 기존 샘플들 사이에서 새로운 합성 샘플을 생성하는 기법
- 소수 클래스 샘플의 k-최근접 이웃을 찾아 이웃들 사이의 선분을 따라 새로운 샘플을 생성하는 방식

### SMOTE 알고리즘 단계

- 소수 클래스의 샘플 x 선택
- x의 k-최근접 이웃 중 하나인 x’ 선택
- x와 x’ 사이에서 합성 샘플 생성
    - 새로운 샘플 = x + r(x’ - x)
    - x와 x’ 사이 어느 지점에 새로운 샘플 생성
- 원하는 합성 샘플이 생성될 때까지 반복

### SMOTE 장점

- 소수 클래스에 합성 샘플을 추가하여 클래스 불균형 문제 해결
- 랜덤 언더샘플링이나 토멕 링크 같은 다른 기법과 결합하여 데이터셋의 균형 개선 가능
- 범주형 데이터와 수치형 데이터 모두 적용 가능

### SMOTE 단점

- 비현실적이거나 잡음이 섞인 합성 샘플을 생성하여 과대적합 초래
- 소수 클래스에 너무 민감한 결정 경계를 생성하여 다수 클래스의 성능 저하
- 대규모 데이터셋에서는 계산 비용 증가

## 2. 니어미스 알고리즘

- 다수 클래스의 데이터를 언더샘플링하여 클래스 간의 분포를 균형있게 맞추는 기법
- 두 클래스 간 데이터가 매우 가까울 때, 다수 클래스에서 일부 데이터를 제거함으로써 두 클래스 간의 거리를 증가시켜 분류 성능 향상

### 최근접 이웃 방법 작동 방식

- 다수 클래스와 소수 클래스의 모든 데이터 간 거리 계산 → 다수 클래스의 데이터 언더샘플링
- 소수 클래스와 가장 가까운 n개의 다수 클래스 데이터 선택
- 소수 클래스에 k개의 데이터 존재 시 니어미스 방법은 다수 클래스에서 kn개의 데이터 반환

### 니어미스 알고리즘 변형

- 소수 클래스의 k-최근접 이웃들과의 평균 거리가 가장 작은 다수 클래스 데이터 선택
- 소수 클래스의 k-가장 먼 이웃들과의 평균 거리가 가장 작은 다수 클래스 데이터 선택
- 소수 클래스의 각 데이터에 대해 M개의 최근접 이웃 저장 → 다수 클래스의 데이터 중 N개의 최근접 이웃과의 평균 거리가 가장 큰 데이터 선택

## 3. 비용 민감 학습

- 클래스별로 서로 다른 오분류 비용을 모델에 부여하여 소수 클래스를 올바르게 분류하는 데 더 집중할 수 있도록 함
- $loss = -(w_{pos} y \log(\hat{y}) + w_{neg}(1 - y)\log(1 - \hat{y}))$

## 4. 데이터 증강

- 원본 데이터에 다양한 변환을 적용하여 새로운 예제를 생성하되, 원래의 레이블은 유지하는 것
- 회전, 이동, 크기 조정, 뒤집기, 노이즈 추가 등
- 한 클래스의 샘플 수가 다른 클래스에 비해 현저히 적은 불균형 데이터셋에 유용

# 8. 상관 계수 다루기

- 시계열 교차 검증 : 데이터를 여러 폴드로 나누되 각 폴드가 연속적인 시간 블록으로 구성되도록 함
- 특성 공학 : 데이터를 모델에 더 적합한 형식으로 변환
- 시계열 전용 모델 : ARIMA, SARIMA, Prophet, LSTM 네트워크 등의 시계열 데이터를 위한 전용 모델
- 시계열 전처리 기법 : 차분, 추세 제거, 정규화 등의 기법 사용
- 차원 축소 기법 : PCA, 오토인코더 등의 차원 축소 기법 사용