# 1. 딥러닝 기본 이해

## 1. 신경망 개요

- 신경망 : 뇌의 구조와 기능에서 영감을 받아 설계된 알고리즘
- 학습 : 특정 작업을 수행하도록 명시적으로 프로그래밍이 되지 않은 상태에서도 경험을 통해 자동으로 학습하고 개선하는 모델의 능력

### 신경망 사용 이유

- 복잡한 관계 파악 능력 : 데이터 속 비선형적 관계 잘 포착
- 뛰어난 근사 능력 : 거의 모든 함수를 높은 정확도로 근사 가능
- 다차원 데이터 처리 : 많은 특징이나 차원을 가진 데이터를 효과적으로 다루기 가능
- 패턴 인식과 예측 : 대규모 데이터에서 패턴과 경향을 찾아내는 데 탁월
- 빠른 처리 속도 : 여러 계산 동시 처리 가능
- 지속적인 학습 능력 : 더 많은 데이터를 접할수록 성능 향상
- 강건성 : 입력 데이터에 노이즈가 있더라도 잘 처리 가능

### 자연어 처리에서의 신경망

- 순차 데이터 처리 : 시퀀스 이전 단계에 대한 내부 상태나 메모리를 유지하며 순차 데이터 처리 가능
- 문맥 이해 : 주변 단어나 이전 문장까지 고려하여 문장의 문맥 이해 가능
- 의미적 해싱 : 단어 임베딩을 통해 단어의 의미를 보존하는 방식으로 인코딩 가능
- 종단간 학습 : 원시 데이터에서 직접 학습 가능
- 성능 : 기계 번역, 텍스트 요약, 감정 분석, 질의응답 등 많은 자연어 처리에서 최고 수준 결과 달성
- 계층적 특성 학습 : 간단한 것을 표현하는 것부터 감성 같은 복잡한 개념 표현 가능

## 2. 신경망의 기본 설계

- 입력층 : 신경망이 입력 데이터를 받는 부분
- 은닉층 : 각 뉴런은 이전 층의 뉴런들로부터 출력을 입력으로 받아 각각의 연결 가중치와 곱한 뒤 이 값을 모두 합산하여 활성화 함수를 통해 비선형성을 도입하여 처리
- 출력층 : 최종 결과를 출력하는 부분

## 3. 신경망의 주요 용어

### 뉴런 또는 노드

- 신경망에서 계산을 수행하는 기본 단위
- 입력, 가중치, 편향, 활성화 함수 등을 통해 간단한 계산 수행
- 각 입력에 가중치를 곱한 후, 이 값들을 모두 합산하고 편향을 더한 다음 활성화 함수 적용
- 가중합 계산 : 뉴런에 들어오는 각 입력에 해당하는 가중치를 곱한 후 이 값들을 모두 더하고 편향 항 추가
- 활성화 함수 : 가중합 결과에 활성화 함수 적용 → 뉴런의 출력에 비선형성 추가

### 가중치와 편향

- 가중치 : 두 뉴런 간 연결의 강도
- 편향 : 뉴런의 추가적인 매개변수, 활성화 함수의 출력을 왼쪽이나 오른쪽으로 이동시켜 학습을 더 유연하게 함

### 활성화 함수

- 각 뉴런이 주어진 입력에 대해 어떤 출력을 생성할지 결정하는 함수
- 시그모이드 함수 : 입력값을 0 또는 1로 구분하여 분류
    - $f(x) = \frac{1}{1 + e^{-x}}$
    - 기울기 소실 문제 : 큰 양수나 음수 입력에서 기울기가 매우 작아짐 → 역전파 학습이 느려짐
- 쌍곡탄젠트 : 입력값을 -1과 1 사이로 압축
    - $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
    - 기울기 소실 문제
- ReLU : 입력이 양수일 때는 그대로 출력, 음수일 때는 0 출력
    - $f(x) = \max(0, x)$
    - 모든 뉴런을 동시에 활성화하지 않음
- Leaky ReLU : 학습 도중 학습을 완전히 멈추는 위험 문제를 해결하기 위해 음수 입력에 대해 작은 선형 값 반환
    - $f(x) = \max(0.01x, x)$
- ELU : ReLU의 변형, 음수 x에 대해 0이 아닌 값을 반환해 학습 과정에서 도움을 줌
    - $f(x) = \begin{cases}x & x > 0 \\ \alpha(e^x - 1) & else\end{cases}$
- 소프트맥스 : 다중 클래스 분류기에서 출력층에 자주 사용, 입력이 각 클래스에 속할 확률 반환
    - $f(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$
    - 모든 클래스에 대한 확률의 합이 1이 되도록 정규화

### 층

- 신경망에서 신호를 같은 수준의 추상화로 처리하는 뉴런들의 집합

### 에폭

- 신경망을 훈련하는 과정에서 전체 훈련 데이터셋을 한 번 완전히 통과하는 과정
- 신경망의 가중치는 손실 함수를 최소화하려는 시도로 업데이트
- 딥러닝 알고리즘이 전체 훈련 데이터셋을 몇 번 처리할지를 결정하는 하이퍼파라미터
- 배치와 미니 배치 방식의 경사하강법에서 더 중요한 개념

### 배치 크기

- 한 번의 반복에서 사용되는 훈련 데이터의 수
- 배치 경사 하강법 : 전체 훈련 데이터셋을 사용하여 각 최적화 단계마다 손실 함수의 기울기 계산
- 확률적 경사 하강법 : 각 최적화 단계마다 하나의 예제만 사용
- 미니 배치 경사 하강법 : 배치 경사 하강법과 확률적 경사 하강법의 절충안

### 배치 관련 추가 내용

- 반복 : 알고리즘이 처리한 데이터 배치의 수
- 학습률 : 손실 기울기에 기반하여 가중치 업데이트 속도를 조절함으로써 학습 알고리즘의 수렴 속도를 제어하는 하이퍼파라미터
- 손실 함수 : 데이터셋에 대한 신경망의 성능 평가
- 역전파 : 신경망에서 경사 하강법을 수행하는 주요 알고리즘
- 드롭아웃 : 학습 중 무작위로 선택된 뉴런을 무시함으로써 과대적합을 방지하는 정규화 기법
- 합성곱 신경망 : 이미지 처리 및 컴퓨터 비전 작업에 적합한 신경망의 한 유형
- 순환 신경망 : 시계열 데이터나 텍스트와 같은 순차 데이터의 패턴을 인식하도록 설계된 신경망 유형

# 2. 다양한 신경망 아키텍처

## 1. 전방향 신경망

- 신경망의 가장 기본적인 형태
- 입력층에서 시작해 은닉층을 거쳐 출력층으로 오직 한 방향으로만 이동
- 순환이나 루프 없이 직선적인 전방향 경로를 따름

## 2. 다층 퍼셉트론

- 전방향 신경망의 일종
- 입력층과 출력층 외에도 최소 하나 이상의 숨겨진 층 포함
- 각 층은 완전히 연결되어 있어 한 층의 모든 뉴런이 다음 층의 모든 뉴런과 연결

## 3. 합성곱 신경망

- 공간적 데이터를 다루는 작업에 적합
- 합성곱 층 : 입력에 여러 필터를 적용하여, 네트워크가 특성의 공간적 계층 구조를 자동으로 학습할 수 있도록 함
- 풀링 층 : 표현의 공간적 크기를 줄여 네트워크의 매개변수와 계산을 줄임으로써 과대적합을 방지하고 이후 층의 계산 비용 감소
- 완전 연결 층 : 풀링 층의 출력을 받아 고차원적인 추론 수행

## 4. 순환 신경망

- 방향성 사이클을 형성하는 연결 보유
- 이전 출력 정보를 입력으로 사용할 수 있게 하여 시계열 예측이나 자연어 처리와 같은 순차적 데이터를 다루는 작업에 적합
- 메모리 셀 : 정보를 장기간 유지할 수 있음

## 5. 오토인코더

- 입력 데이터를 효율적으로 인코딩하는 방법을 학습하는 신경망의 한 유형
- 대칭적인 아키텍처 보유, 역전파를 통해 목표값을 입력값과 동일하게 설정하도록 설계
- 특성 추출, 데이터 표현 학습, 차원 축소, 생성 모델, 잡음 제거, 추천 시스템 등

## 6. 생성적 적대 신경망

- 생성자 : 훈련 데이터셋과 동일한 분포에서 생성된 것처럼 보이는 데이터 인스턴스 생성
- 판별자 : 실제 분포에서 온 인스턴스와 생성자가 만든 인스턴스 구별
- 생성자가 더 나은 인스턴스 생성, 판별자는 실제 인스턴스와 생성된 인스턴스를 더 잘 구별하도록 학습

# 3. 신경망 훈련의 도전 과제

- 지역 최솟값 : 손실 함수를 최소화하는 가중치 세트 찾기
- 기울기 소실 : 가중치 업데이트가 너무 작아져서 네트워크가 데이터를 제대로 학습하지 못하는 것
- 기울기 폭주 : 가중치 업데이트가 너무 커져 손실이 정의되지 않는 값이 되어 학습 실패
- 과대적합 : 모델이 너무 복잡하거나 과도하게 학습된 경우
- 과소적합 : 모델이 너무 단순하여 데이터의 기본 구조를 제대로 파악하지 못한 경우
- 계산 자원 : 심층 신경망을 학습시키기 위해 많은 계산 자원 필요
- 해석 가능성 부족 : 블랙박스 → 왜 특정 예측을 했는지 이해하기 어려움
- 적절한 아키텍처 및 하이퍼파라미터 선택 어려움 : 주어진 문제에 대해 최적의 아키텍처와 하이퍼파라미터 조정 어려움
- 데이터 전처리 : 입력 데이터를 특정 형식으로 요구

# 4. 언어 모델

- 인간 언어의 구조를 학습하고 이해하도록 설계된 자연어 처리의 통계적 모델
- 주어진 단어의 시나리오에서 다음에 나올 단어의 가능성을 추정하도록 학습된 확률적 모델

### 언어 모델 사용 이유

- 기계 번역 : 번역된 문장의 유창성 평가 및 여러 가능한 번역 중 더 적합한 번역 선택
- 음성 인식 : 비슷하게 들리는 단어와 구를 구별하는 데 사용
- 정보 검색 : 쿼리와 관련된 문서를 결정하는 데 도움
- 텍스트 생성 : 인간과 유사한 텍스트 생성 가능
- 감정 분석 : 텍스트의 감정이 긍정인지, 부정인지, 중립인지 판단 가능
- 문법 검사 : 문장에서 다음에 나올 단어를 예측하여 문법 오류나 어색한 표현 식별
- 개체명 인식 : 텍스트에서 사람, 조직, 위치 등과 같은 개체명 인식 가능
- 문맥 이해 : 단어와 문장의 문맥 이해 가능

## 1. 자기 지도 학습을 사용한 언어 모델 훈련

- 데이터 자체가 감독을 제공하는 비지도 학습의 한 형태
- 같은 입력 데이터의 다른 부분에서 특정 부분 예측 학습
- 문장의 일부가 주어졌을 때 다른 부분을 에측하는 방식으로 구현

### 마스크 언어 모델링

- BERT 훈련에 사용
- 입력 토큰의 일정 비율을 무작위로 마스킹하고 마스킹되지 않은 단어들이 제공하는 문맥을 바탕으로 마스킹된 단어 예측
- $L = \sum_i \log P(w_i \mid w_{i - 1}; \theta)$

### 자기회귀 언어 모델링

- GPT와 같은 모델에 사용
- 문장의 이전 모든 단어를 기반으로 다음 단어를 예측하는 방식
- 주어진 문맥에서 다음에 올 단어의 가능도 최대화
- $L = \sum_i \log P(w_i \mid w_1, \cdots, w_{i - 1}; \theta)$
- 명시적인 레이블 없이 원시 텍스트에서 직접 언어의 구문과 의미에 대한 풍부한 이해 습득 가능

## 2. 전이학습

- 사전 훈련된 모델을 다른 유사한 문제의 출발점으로 재사용하는 머신러닝 기법
- 관련 작업에서 학습된 패턴을 바탕으로 학습 시작 가능
- 대규모 작업에서 모델 먼저 학습 → 모델의 일부를 다른 관련 작업의 시작점으로 활용
- 두 작업의 입력 데이터 유형이 같고 작업 간 유사성 존재 시 효과적

### 특성 추출

- 모델의 마지막 층 또는 여러 층을 제거하고 나머지 네트워크 유지
- 데이터를 축소된 모델에 통과시켜 나온 출력을 새로운 더 작은 모델의 입력으로 사용 → 해당 모델을 특정 작업에 맞게 학습

### 미세 조정

- 사전 훈련된 모델을 출발점으로 사용하고 새로운 작업을 위해 모델의 모든 또는 일부 매개변수 업데이트
- 훈련이 중단된 지점부터 계속 진행 → 모델이 일반적인 특징 추출에서 새로운 작업에 더 특화된 특징으로 조정

# 5. 트랜스포머 이해하기

- 셀프 어텐션 메커니즘 : 모델이 출력을 생성할 때 입력에 있는 각 단어의 중요도를 가중치로 두어 각 단어의 문맥을 고려할 수 있도록 함

## 1. 트랜스포머 아키텍처

- 인코더
    - 셀프 어텐션 메커니즘 + 위치별 완전 연결 순방향 신경망
    - 입력값을 출력에 더해 주는 잔차 연결 → 층 정규화
- 디코더
    - 셀프 어텐션 층 + 교차 어텐션 층 + 위치별 완전 연결 순방향 신경망
    - 잔차 연결 → 층 정규화

## 셀프 어텐션 메커니즘

- 현재 처리 중인 단어와 시퀀스 내의 각 단어 간의 관련성을 계산하는 방식
- 입력 : 단어 임베딩의 시퀀스
- $\text{어텐션}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$
- 현재 단어의 출력을 생성할 때 각 단어의 값에 부여되는 가중치 표현
- 출력 : 새로운 벡터 시퀀스, 모든 입력의 가중합으로 계산

### 위치 인코딩

- 위치의 고정된 함수, 모델이 단어의 순서를 학섭할 수 있도록 함
- 입력 임베딩에 위치 정보를 더해주는 것

# 6. 대규모 언어 모델

- 다양한 인터넷 텍스트를 학습한 머신러닝 모델의 한 유형
- 방대한 텍스트 코퍼스에서 자기 지도 학습을 통해 학습
- 문법, 세계에 대한 지식, 추론 능력 및 데이터에 내재한 편향까지 학습
- 트랜스포머 아키텍처 기반 + 셀프 어텐션 메커니즘 사용

# 7. 언어 모델 훈련의 도전 과제

- 계산 자원 : 대규모 언어 모델을 학습시키기 위한 막대한 계산 자원 필요
- 메모리 한계 : 모델의 크기가 커질수록 학습 중 모델 매개변수, 중간 활성화 값, 기울기를 저장하는 데 필요한 메모리 양 증가
- 데이터셋 크기와 품질 : 방대한 말뭉치에서 학습 → 데이터에 존재하는 편향이나 오류도 학습 가능
- 과대적합 : 복잡한 패턴을 학습할 수 있는 높은 용량 보유, 학습 데이터의 양이 모델 크기에 비해 제한적일 경우 과대적합 발생
- 학습 안정성 : 학습률과 배치 크기 관리, 기울기 소실 및 폭주 문제 처리 어려움
- 평가와 미세 조정 : 특정 작업에 맞게 미세 조정하는 과정에서 재앙적 망각 문제 발생
- 윤리적 안전 문제 : 해롭거나 부적절한 콘텐츠 생성 가능

## 1. 언어 모델의 특정 설계

### BERT

- 자연어 처리 작업을 위한 트랜스포머 기반 머신러닝 기법
- 모든 층에서 왼쪽과 오른쪽 문맥을 동시에 고려하여 레이블이 없는 텍스트로부터 깊이 있는 양방향 표현을 사전 훈련하도록 설계
- 셀프 어텐션과 점별 완전 연결 층으로 구성
- 사전 훈련 → 미세 조정

### 토크나이저

- 입력 텍스트를 제한된 수의 토큰으로 변환
- 바이트 페어 인코딩, 유니그램 언어 모델, 워드피스
- 어휘 외 단어 처리 및 서브워드 부분에 대한 유의미한 표현 학습 가능
- 텍스트 데이터를 모델에 입력하기 전에 필요한 초기 전처리 수행
- 기본 토크나이징 : 텍스트를 공백과 구두점을 기준으로 개별 단어로 분할
- 워드피스 토크나이징 : 단어를 더 작은 서브워드 단위 또는 워드피스로 분할
- 특수 토큰 추가 : 특정 BERT 기능에 필요한 특수 토큰 추가([CLS], [SEP])
- 토큰을 ID로 변환 : 각 토큰은 BERT 어휘에서 인덱스에 해당하는 정수 ID로 대치

### 사전 훈련

- 방대한 텍스트 코퍼스 기반으로 학습
- 마스크된 언어 모델 : 문장의 15%에 해당하는 단어를 [MASK] 토큰으로 대체, 마스크되지 않은 단어들로 제공된 문맥을 기반으로 원래의 단어 예측
- 다음 문장 예측 : 두 개의 문장이 주어졌을 때, 문장 B가 문장 A의 다음 문장인지 예측

### 미세 조정

- 특정 작업에 대해 훨씬 적은 양의 학습 데이터로 미세 조정
- BERT에 추가 출력층을 더한 후 특정 작업에 대해 전체 모델을 처음부터 끝까지 학습