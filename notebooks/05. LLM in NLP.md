# 1. 대규모 언어 모델과 기존 언어 모델의 차이

## 1. N-그램 모델

- (n - 1)개의 이전 단어를 사용하여 n번째 단어 예측
- 간단한 구현 및 높은 계산 효율
- 단어 간 장기 의존성 포착 어려움
- 데이터 희소성 문제로 인한 성능 저하 발생

## 2. 은닉 마르코프 모델(HMM)

- 데이터를 생성하는 숨겨진 상태 고려
- 관찰된 상태 : 각 단어
- 숨겨진 상태 : 직접 관찰할 수 없는 언어적 특징
- 단어 간 장기적인 의존성 포착 어려움

## 3. 순환 신경망

- 노드 간의 연결이 시간 순서에 따라 방향성 그래프를 형성하는 신경망의 한 종류
- 내부 상태를 사용해 시퀀스 데이터 처리 가능

### 장단기 메모리 네트워크(LSTM)

- 장기적인 의존성을 학습하도록 설계
- 네트워크의 메모리 상태로의 정보 흐름을 제어하는 게이트를 사용하여 정보 유지 및 휘발 조절

### 게이트 순환 유닛 네트워크(GRU)

- LSTM의 변형, 조금 다른 게이트 구조 사용

# 2. 트랜스포머

- 입력 텍스트 내 단어들 간의 관계를 효과적으로 파악하는 셀프 어텐션 메커니즘 사용
- 각 토큰의 표현이 시퀀스 내 모든 토큰의 가중치 합으로 계산
- 모든 토큰을 동시에 고려 → 토큰 간 장기적 의존성 더 잘 포착 가능